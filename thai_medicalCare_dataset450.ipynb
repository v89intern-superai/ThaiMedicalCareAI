{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "T8VzLTcsQnvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fYECvggLhPy"
      },
      "outputs": [],
      "source": [
        "# Thai Medical Care Dataset Translator - Version 2.2\n",
        "# Maximum efficiency with Google CPU\n",
        "\n",
        "!pip install -q datasets googletrans==4.0.0-rc1 tqdm requests deep-translator\n",
        "\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from googletrans import Translator\n",
        "from deep_translator import GoogleTranslator\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import time\n",
        "import logging\n",
        "import json\n",
        "import requests\n",
        "from typing import Optional, List, Dict, Any\n",
        "import random\n",
        "import hashlib\n",
        "import threading\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Set up logging with shortened warnings\n",
        "logging.basicConfig(level=logging.INFO, format='%(levelname)s:%(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class RobustTranslator:\n",
        "    def __init__(self):\n",
        "        self.translators = []\n",
        "        self.cache = {}\n",
        "        self.max_text_length = 3500  # Optimized for T4 GPU\n",
        "        self.rate_limit_delay = 1.5   # Reduced delay for efficiency\n",
        "        self.last_request_time = 0\n",
        "        self.request_lock = threading.Lock()\n",
        "        self._initialize_translators()\n",
        "\n",
        "    def _initialize_translators(self):\n",
        "        \"\"\"Initialize translators with fixed error handling\"\"\"\n",
        "        try:\n",
        "            # Fix: Proper googletrans initialization\n",
        "            gt = Translator()\n",
        "            # Test translation without calling non-existent method\n",
        "            test_result = gt.translate(\"test\", src='en', dest='th')\n",
        "            if test_result and hasattr(test_result, 'text') and test_result.text:\n",
        "                self.translators.append({'name': 'googletrans', 'obj': gt})\n",
        "                logger.info(\"✅ googletrans ready\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"googletrans failed: {str(e)[:50]}\")\n",
        "\n",
        "        try:\n",
        "            # Fix: Proper deep-translator initialization\n",
        "            dt = GoogleTranslator(source='en', target='th')\n",
        "            test_result = dt.translate(\"test\")\n",
        "            if test_result and isinstance(test_result, str) and test_result.strip():\n",
        "                self.translators.append({'name': 'deep_translator', 'obj': dt})\n",
        "                logger.info(\"✅ deep-translator ready\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"deep-translator failed: {str(e)[:50]}\")\n",
        "\n",
        "        if not self.translators:\n",
        "            raise RuntimeError(\"❌ No translators initialized!\")\n",
        "\n",
        "        logger.info(f\"✅ {len(self.translators)} translator(s) active\")\n",
        "\n",
        "    def _get_cache_key(self, text: str) -> str:\n",
        "        \"\"\"Generate cache key\"\"\"\n",
        "        return hashlib.md5(text.encode('utf-8')).hexdigest()[:16]  # Shorter keys for efficiency\n",
        "\n",
        "    def _apply_rate_limit(self):\n",
        "        \"\"\"Thread-safe rate limiting optimized for T4 GPU\"\"\"\n",
        "        with self.request_lock:\n",
        "            current_time = time.time()\n",
        "            time_diff = current_time - self.last_request_time\n",
        "            if time_diff < self.rate_limit_delay:\n",
        "                sleep_time = self.rate_limit_delay - time_diff + random.uniform(0.1, 0.3)\n",
        "                time.sleep(sleep_time)\n",
        "            self.last_request_time = time.time()\n",
        "\n",
        "    def _clean_text(self, text: str) -> Optional[str]:\n",
        "        \"\"\"Fast text cleaning for maximum efficiency\"\"\"\n",
        "        if not text or pd.isna(text):\n",
        "            return None\n",
        "\n",
        "        text = str(text).strip()\n",
        "        if not text or text in ['', 'nan', 'None', 'null']:\n",
        "            return None\n",
        "\n",
        "        # Quick cleanup\n",
        "        text = ' '.join(text.split())\n",
        "\n",
        "        # Efficient truncation for long text\n",
        "        if len(text) > self.max_text_length:\n",
        "            # Find last complete sentence within limit\n",
        "            truncate_pos = self.max_text_length\n",
        "            for delimiter in ['. ', '? ', '! ']:\n",
        "                pos = text.rfind(delimiter, 0, self.max_text_length)\n",
        "                if pos > self.max_text_length * 0.7:  # At least 70% of max length\n",
        "                    truncate_pos = pos + len(delimiter)\n",
        "                    break\n",
        "            text = text[:truncate_pos].strip()\n",
        "\n",
        "        return text if len(text.strip()) > 0 else None\n",
        "\n",
        "    def _translate_googletrans(self, text: str) -> Optional[str]:\n",
        "        \"\"\"Fixed googletrans translation\"\"\"\n",
        "        try:\n",
        "            translator = next((t['obj'] for t in self.translators if t['name'] == 'googletrans'), None)\n",
        "            if not translator:\n",
        "                return None\n",
        "\n",
        "            self._apply_rate_limit()\n",
        "\n",
        "            # Fix: Removed the non-existent raise_Exception call\n",
        "            result = translator.translate(text, src='en', dest='th')\n",
        "\n",
        "            if result and hasattr(result, 'text') and result.text:\n",
        "                translated = result.text.strip()\n",
        "                if translated and translated != text:\n",
        "                    return translated\n",
        "\n",
        "        except Exception as e:\n",
        "            # Shortened error messages as requested\n",
        "            logger.debug(f\"GT failed: {str(e)[:30]}\")\n",
        "\n",
        "        return None\n",
        "\n",
        "    def _translate_deep_translator(self, text: str) -> Optional[str]:\n",
        "        \"\"\"Fixed deep-translator translation\"\"\"\n",
        "        try:\n",
        "            translator = next((t['obj'] for t in self.translators if t['name'] == 'deep_translator'), None)\n",
        "            if not translator:\n",
        "                return None\n",
        "\n",
        "            self._apply_rate_limit()\n",
        "\n",
        "            # Fix: Proper error handling without non-existent methods\n",
        "            result = translator.translate(text)\n",
        "\n",
        "            if result and isinstance(result, str):\n",
        "                translated = result.strip()\n",
        "                if translated and translated != text:\n",
        "                    return translated\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.debug(f\"DT failed: {str(e)[:30]}\")\n",
        "\n",
        "        return None\n",
        "\n",
        "    def translate_text(self, text: str, max_retries: int = 2) -> str:  # Reduced retries for efficiency\n",
        "        \"\"\"Optimized translation with bug fixes\"\"\"\n",
        "        cleaned_text = self._clean_text(text)\n",
        "        if not cleaned_text:\n",
        "            return \"\"\n",
        "\n",
        "        # Cache check\n",
        "        cache_key = self._get_cache_key(cleaned_text)\n",
        "        if cache_key in self.cache:\n",
        "            return self.cache[cache_key]\n",
        "\n",
        "        # Fixed translation methods without problematic calls\n",
        "        methods = [self._translate_googletrans, self._translate_deep_translator]\n",
        "\n",
        "        # Optimized retry logic\n",
        "        for attempt in range(max_retries):\n",
        "            for method in methods:\n",
        "                try:\n",
        "                    result = method(cleaned_text)\n",
        "                    if result and result.strip():\n",
        "                        self.cache[cache_key] = result\n",
        "                        return result\n",
        "                except Exception as e:\n",
        "                    continue\n",
        "\n",
        "            # Shorter backoff for efficiency\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(1 + random.uniform(0.2, 0.8))\n",
        "\n",
        "        # Shortened error format as requested\n",
        "        short_text = cleaned_text[:20] + \"...\" if len(cleaned_text) > 20 else cleaned_text\n",
        "        return f\"[FAILED]: {short_text}\"\n",
        "\n",
        "class DatasetTranslator:\n",
        "    def __init__(self, max_workers: int = 2):  # Optimized for T4 GPU\n",
        "        self.translator = RobustTranslator()\n",
        "        self.max_workers = max_workers\n",
        "\n",
        "    def translate_batch(self, texts: List[str], column_name: str) -> List[str]:\n",
        "        \"\"\"Optimized batch translation with shortened progress messages\"\"\"\n",
        "        if not texts:\n",
        "            return []\n",
        "\n",
        "        results = [''] * len(texts)\n",
        "        valid_texts = [(i, text) for i, text in enumerate(texts) if text and str(text).strip()]\n",
        "\n",
        "        logger.info(f\"Translating {column_name}: {len(valid_texts)} items\")\n",
        "\n",
        "        # Optimized single-threaded processing for stability\n",
        "        with tqdm(valid_texts, desc=f\"{column_name}\") as pbar:\n",
        "            for i, text in pbar:\n",
        "                try:\n",
        "                    translated = self.translator.translate_text(str(text))\n",
        "                    results[i] = translated\n",
        "\n",
        "                    # Shortened progress updates\n",
        "                    if (i + 1) % 10 == 0 or i == len(valid_texts) - 1:\n",
        "                        success_count = sum(1 for r in results[:i+1] if r and not r.startswith(\"[FAILED]\"))\n",
        "                        pbar.set_postfix({'OK': f\"{success_count}/{i+1}\"})\n",
        "\n",
        "                except Exception as e:\n",
        "                    # Shortened error messages\n",
        "                    short_text = str(text)[:10] + \"...\"\n",
        "                    logger.warning(f\"{short_text}\")\n",
        "                    results[i] = f\"[ERROR]: {short_text}\"\n",
        "\n",
        "        # Quick retry for critical failures only\n",
        "        failed_indices = [i for i, r in enumerate(results) if r.startswith(\"[FAILED]\") or r.startswith(\"[ERROR]\")]\n",
        "\n",
        "        if failed_indices and len(failed_indices) < len(valid_texts) * 0.3:  # Only if < 30% failed\n",
        "            logger.info(f\"Retrying {len(failed_indices)} failed items...\")\n",
        "\n",
        "            for i in failed_indices[:10]:  # Limit retries for efficiency\n",
        "                try:\n",
        "                    time.sleep(2)  # Short delay\n",
        "                    original_text = texts[i]\n",
        "                    retranslated = self.translator.translate_text(str(original_text), max_retries=1)\n",
        "\n",
        "                    if not retranslated.startswith(\"[FAILED]\") and not retranslated.startswith(\"[ERROR]\"):\n",
        "                        results[i] = retranslated\n",
        "\n",
        "                except Exception:\n",
        "                    continue\n",
        "\n",
        "        # Quick stats\n",
        "        total_processed = len([r for r in results if r])\n",
        "        successful = len([r for r in results if r and not r.startswith(\"[\")])\n",
        "        success_rate = (successful / total_processed * 100) if total_processed > 0 else 0\n",
        "\n",
        "        logger.info(f\"✅ {column_name}: {successful}/{total_processed} ({success_rate:.0f}%)\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def process_dataset(self, dataset_name: str, split: str = 'train', sample_size: int =450,\n",
        "                       columns_to_translate: List[str] = None) -> Optional[pd.DataFrame]:\n",
        "        \"\"\"Optimized dataset processing with bug fixes\"\"\"\n",
        "        logger.info(f\"📊 Processing: {dataset_name}\")\n",
        "\n",
        "        try:\n",
        "            # Fix: Better dataset loading with proper error handling\n",
        "            dataset = None\n",
        "            for trust_remote in [True, False]:\n",
        "                try:\n",
        "                    dataset = load_dataset(dataset_name, split=split, trust_remote_code=trust_remote)\n",
        "                    break\n",
        "                except Exception as e:\n",
        "                    if \"raise_Exception\" in str(e):\n",
        "                        # Skip this specific error pattern\n",
        "                        continue\n",
        "                    logger.warning(f\"Load attempt failed: {str(e)[:50]}\")\n",
        "                    continue\n",
        "\n",
        "            if dataset is None:\n",
        "                logger.error(f\"❌ Failed to load {dataset_name}\")\n",
        "                return None\n",
        "\n",
        "            # Efficient sampling\n",
        "            actual_size = min(sample_size, len(dataset))\n",
        "\n",
        "            # Fix: Improved pandas conversion with proper error handling\n",
        "            try:\n",
        "                if actual_size == len(dataset):\n",
        "                    df = dataset.to_pandas()\n",
        "                else:\n",
        "                    df = dataset.select(range(actual_size)).to_pandas()\n",
        "            except Exception as e:\n",
        "                # Alternative method for problematic datasets\n",
        "                logger.info(\"Using alternative conversion...\")\n",
        "                data_dict = {}\n",
        "                sample_data = dataset.select(range(actual_size))\n",
        "                for key in sample_data.features.keys():\n",
        "                    try:\n",
        "                        data_dict[key] = [example[key] for example in sample_data]\n",
        "                    except Exception:\n",
        "                        data_dict[key] = [\"\" for _ in range(actual_size)]  # Fill with empty strings\n",
        "                df = pd.DataFrame(data_dict)\n",
        "\n",
        "            logger.info(f\"✅ Loaded {len(df)} samples\")\n",
        "            logger.info(f\"Columns: {list(df.columns)}\")\n",
        "\n",
        "            # Smart column mapping\n",
        "            if columns_to_translate:\n",
        "                available_columns = set(df.columns)\n",
        "                columns_map = {}\n",
        "\n",
        "                for target_col in columns_to_translate:\n",
        "                    if target_col in available_columns:\n",
        "                        columns_map[target_col] = target_col\n",
        "                    else:\n",
        "                        # Find similar columns\n",
        "                        matches = [col for col in available_columns\n",
        "                                 if target_col.lower() in col.lower() or col.lower() in target_col.lower()]\n",
        "                        if matches:\n",
        "                            best_match = min(matches, key=len)\n",
        "                            columns_map[target_col] = best_match\n",
        "                            logger.info(f\"Mapped '{target_col}' → '{best_match}'\")\n",
        "\n",
        "                # Translate available columns\n",
        "                for target_col, actual_col in columns_map.items():\n",
        "                    if actual_col in df.columns:\n",
        "                        logger.info(f\"🔄 Translating '{actual_col}'...\")\n",
        "\n",
        "                        text_values = df[actual_col].fillna(\"\").astype(str).tolist()\n",
        "                        non_empty_count = sum(1 for val in text_values if val and str(val).strip() not in ['', 'nan', 'None'])\n",
        "\n",
        "                        if non_empty_count > 0:\n",
        "                            translated_values = self.translate_batch(text_values, actual_col)\n",
        "                            translated_col_name = f'th_{actual_col}'\n",
        "                            df[translated_col_name] = translated_values\n",
        "\n",
        "                            successful_translations = sum(\n",
        "                                1 for val in translated_values\n",
        "                                if val and not val.startswith(\"[\")\n",
        "                            )\n",
        "                            logger.info(f\"✅ Added '{translated_col_name}' with {successful_translations} translations\")\n",
        "                        else:\n",
        "                            logger.warning(f\"⚠️ No valid text in '{actual_col}'\")\n",
        "\n",
        "            return df\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"❌ Error processing {dataset_name}: {str(e)[:100]}\")\n",
        "            return None\n",
        "\n",
        "    def save_dataset(self, df: pd.DataFrame, filename: str, drive_path: str) -> bool:\n",
        "        \"\"\"Optimized dataset saving\"\"\"\n",
        "        if df is None or df.empty:\n",
        "            logger.error(\"❌ Cannot save empty dataset\")\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            os.makedirs(drive_path, exist_ok=True)\n",
        "            file_path = os.path.join(drive_path, filename)\n",
        "\n",
        "            # Efficient saving with proper Thai encoding\n",
        "            df.to_csv(file_path, index=False, encoding='utf-8-sig', escapechar='\\\\')\n",
        "\n",
        "            if os.path.exists(file_path):\n",
        "                file_size = os.path.getsize(file_path)\n",
        "                logger.info(f\"💾 Saved {len(df)} samples ({file_size:,} bytes)\")\n",
        "                return True\n",
        "            else:\n",
        "                logger.error(f\"❌ File not created: {filename}\")\n",
        "                return False\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"❌ Save error: {str(e)[:50]}\")\n",
        "            return False\n",
        "\n",
        "def main():\n",
        "    \"\"\"Optimized main execution for maximum T4 GPU efficiency\"\"\"\n",
        "    logger.info(\"🚀 Thai Medical Dataset Translation v2.1 - T4 Optimized\")\n",
        "    logger.info(\"=\"*60)\n",
        "\n",
        "    try:\n",
        "        translator = DatasetTranslator(max_workers=2)\n",
        "        drive_path = \"/content/drive/MyDrive/V89Technology/thai_medicalCare_dataset450\"\n",
        "\n",
        "        # Optimized dataset config\n",
        "        datasets_config = [\n",
        "            {\n",
        "                \"name\": \"Amod/mental_health_counseling_conversations\",\n",
        "                \"columns\": ['Context', 'Response'],\n",
        "                \"filename\": \"mental_health_thai_450.csv\"\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"lavita/ChatDoctor-HealthCareMagic-100k\",\n",
        "                \"columns\": ['instruction', 'input', 'output'],\n",
        "                \"filename\": \"healthcare_thai_450.csv\"\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"medalpaca/medical_meadow_pubmed_causal\",\n",
        "                \"columns\": ['input', 'output', 'instruction'],\n",
        "                \"filename\": \"pubmed_thai_450.csv\"\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"medalpaca/medical_meadow_mediqa\",\n",
        "                \"columns\": ['instruction', 'input', 'output'],\n",
        "                \"filename\": \"medical_qa_thai_450.csv\"\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        successful = 0\n",
        "        failed = []\n",
        "        start_time = time.time()\n",
        "\n",
        "        for i, config in enumerate(datasets_config):\n",
        "            logger.info(f\"\\n[{i+1}/{len(datasets_config)}] {config['name']}\")\n",
        "            logger.info(\"-\"*50)\n",
        "\n",
        "            try:\n",
        "                df = translator.process_dataset(\n",
        "                    config[\"name\"],\n",
        "                    sample_size=450,\n",
        "                    columns_to_translate=config[\"columns\"]\n",
        "                )\n",
        "\n",
        "                if df is not None and not df.empty:\n",
        "                    if translator.save_dataset(df, config[\"filename\"], drive_path):\n",
        "                        successful += 1\n",
        "                        logger.info(f\"✅ SUCCESS: {config['name']}\")\n",
        "                    else:\n",
        "                        failed.append(config[\"name\"])\n",
        "                        logger.error(f\"❌ SAVE FAILED: {config['name']}\")\n",
        "                else:\n",
        "                    failed.append(config[\"name\"])\n",
        "                    logger.error(f\"❌ PROCESS FAILED: {config['name']}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                failed.append(config[\"name\"])\n",
        "                logger.error(f\"❌ ERROR: {config['name']}: {str(e)[:50]}\")\n",
        "\n",
        "            # Reduced inter-dataset delay for efficiency\n",
        "            if i < len(datasets_config) - 1:\n",
        "                time.sleep(10)\n",
        "\n",
        "        # Final summary\n",
        "        total_time = time.time() - start_time\n",
        "        success_rate = successful / len(datasets_config) * 100\n",
        "\n",
        "        logger.info(f\"\\n{'='*60}\")\n",
        "        logger.info(\"🏁 TRANSLATION COMPLETE\")\n",
        "        logger.info(f\"⏱️ Total time: {total_time/60:.1f} minutes\")\n",
        "        logger.info(f\"✅ Success: {successful}/{len(datasets_config)} ({success_rate:.0f}%)\")\n",
        "\n",
        "        if failed:\n",
        "            logger.info(f\"❌ Failed: {', '.join(failed)}\")\n",
        "\n",
        "        logger.info(f\"📁 Output: {drive_path}\")\n",
        "        logger.info(\"=\"*60)\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"❌ Fatal error: {str(e)[:100]}\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. Read CSV from Google Drive\n",
        "drive_path = \"/content/drive/MyDrive/V89Technology/thai_medicalCare_dataset450/pubmed_thai_450.csv\"\n",
        "df = pd.read_csv(drive_path)\n",
        "df.info()"
      ],
      "metadata": {
        "id": "qKoaOAaX-K9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "OW2-t25KBwaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = pd.read_csv (\"/content/drive/MyDrive/V89Technology/thai_medicalCare_dataset450/mental_health_thai_450.csv\")\n",
        "df2.info()"
      ],
      "metadata": {
        "id": "ovsgUKoNC94m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3 = pd.read_csv (\"/content/drive/MyDrive/V89Technology/thai_medicalCare_dataset450/medical_qa_thai_450.csv\")\n",
        "df3.info()"
      ],
      "metadata": {
        "id": "8n7bAMWOExvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df4 =  pd.read_csv (\"/content/drive/MyDrive/V89Technology/thai_medicalCare_dataset450/healthcare_thai_450.csv\")\n",
        "df4.info()"
      ],
      "metadata": {
        "id": "vZHQSIe4E9Vt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df4"
      ],
      "metadata": {
        "id": "fPPRKH3m_niu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df4.columns.tolist())\n"
      ],
      "metadata": {
        "id": "41CKgNrSJdMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df4.dropna(subset=['th_input'], inplace=True)\n",
        "df4.info()"
      ],
      "metadata": {
        "id": "WvufUf5KFOB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Replace \"[FAILED]: \" with \"\" across all string columns\n",
        "#df4 = df.applymap(lambda x: x.replace(\"[FAILED]: \", \"\") if isinstance(x, str) else x)\n",
        "\n",
        "# 3. Save the cleaned file to new CSV\n",
        "output_path = \"/content/drive/MyDrive/V89Technology/thai_medicalCare_dataset450/healthcare_thai_449_clean.csv\"\n",
        "df4.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"✅ Cleaned CSV saved at: {output_path}\")\n"
      ],
      "metadata": {
        "id": "866toTr_BZhe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}