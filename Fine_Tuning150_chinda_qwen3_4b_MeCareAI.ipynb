{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-753D09Z0tu"
      },
      "outputs": [],
      "source": [
        "# Enhanced Fine-tuning iapp/chinda-qwen3-4b for Thai Medical Applications\n",
        "# Company: V89 Technology Ltd.\n",
        "# Version: 4.0 - FIXED All Critical Issues with A100 Optimization\n",
        "\n",
        "print(\"Starting Enhanced Chinda-Qwen3-4B Medical Fine-tuning Setup v4.0...\")\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import os\n",
        "import re\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
        "#os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "# Compatible package versions for A100 with Python 3.12.11\n",
        "!pip install -q --upgrade pip==24.0\n",
        "!pip install -q --force-reinstall torch==2.5.0 torchvision==0.20.0 torchaudio==2.5.0 --index-url https://download.pytorch.org/whl/cu124\n",
        "!pip install -q transformers==4.52.3 accelerate==1.1.0 bitsandbytes==0.43.3\n",
        "!pip install -q datasets==2.20.0 evaluate==0.4.2 rouge-score==0.1.2\n",
        "!pip install -q peft==0.12.0\n",
        "!pip install -q scikit-learn pandas numpy==2.0.2 scipy>=1.14.1 fsspec filelock typing-extensions nltk\n",
        "!pip install -q sacrebleu\n",
        "\n",
        "# Import required libraries\n",
        "import sys\n",
        "import json\n",
        "import gc\n",
        "import torch\n",
        "import transformers\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from datasets import Dataset, DatasetDict\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from typing import Dict, List, Any, Optional, Union\n",
        "import torch.nn.functional as F\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "import math\n",
        "import torch.optim as optim\n",
        "from sacrebleu import corpus_bleu\n",
        "from collections import defaultdict\n",
        "\n",
        "# Updated imports for compatibility - FIXED accelerate unwrap_model issue\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    EarlyStoppingCallback,\n",
        "    GenerationConfig,\n",
        "    get_linear_schedule_with_warmup,\n",
        "    TrainerCallback,\n",
        "    TrainerControl,\n",
        "    TrainerState,\n",
        "    EvalPrediction\n",
        ")\n",
        "from peft import (\n",
        "    get_peft_model,\n",
        "    LoraConfig,\n",
        "    TaskType,\n",
        "    PeftModel,\n",
        "    prepare_model_for_kbit_training,\n",
        "    get_peft_config,\n",
        "    PeftType\n",
        ")\n",
        "\n",
        "print(f\"===== Version Check =====\")\n",
        "print(f\"Python version: {sys.version}\")\n",
        "print(\"Transformers version:\", transformers.__version__)\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Numpy version: {np.__version__}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA Device: {torch.cuda.get_device_name()}\")\n",
        "    print(f\"CUDA Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "print(f\"==================\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive with error handling\n",
        "def setup_drive_connection():\n",
        "    \"\"\"Setup Google Drive connection with fallback\"\"\"\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        drive_path = \"/content/drive/MyDrive/V89Technology/thai_medicalCare_dataset150\"\n",
        "        print(\"Google Drive connected successfully\")\n",
        "        return drive_path\n",
        "    except:\n",
        "        local_path = \"V89Technology/thai_medicalCare_dataset150\"\n",
        "        os.makedirs(local_path, exist_ok=True)\n",
        "        print(\"Running outside Colab - using local directory\")\n",
        "        return local_path\n",
        "\n",
        "drive_path = setup_drive_connection()\n",
        "\n",
        "# Enhanced GPU memory management with A100 optimizations\n",
        "class GPUMemoryManager:\n",
        "    \"\"\"Advanced GPU memory management class with A100 optimizations\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def clear_memory():\n",
        "        \"\"\"Comprehensive GPU memory clearing\"\"\"\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            torch.cuda.synchronize()\n",
        "            torch.cuda.reset_peak_memory_stats()\n",
        "            torch.cuda.reset_accumulated_memory_stats()\n",
        "\n",
        "    @staticmethod\n",
        "    def get_memory_info():\n",
        "        \"\"\"Get current GPU memory usage\"\"\"\n",
        "        if torch.cuda.is_available():\n",
        "            allocated = torch.cuda.memory_allocated() / 1024**3\n",
        "            cached = torch.cuda.memory_reserved() / 1024**3\n",
        "            total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "\n",
        "            return {\n",
        "                \"allocated_gb\": allocated,\n",
        "                \"cached_gb\": cached,\n",
        "                \"total_gb\": total,\n",
        "                \"free_gb\": total - allocated\n",
        "            }\n",
        "        return {\"error\": \"CUDA not available\"}\n",
        "\n",
        "    @staticmethod\n",
        "    def optimize_for_device():\n",
        "        \"\"\"Optimized settings for A100 40GB\"\"\"\n",
        "        if torch.cuda.is_available():\n",
        "            device_name = torch.cuda.get_device_name().lower()\n",
        "            memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "\n",
        "            if \"a100\" in device_name:\n",
        "                return {\n",
        "                    \"batch_size\": 16,\n",
        "                    \"gradient_accumulation_steps\": 4,\n",
        "                    \"max_length\": 512,\n",
        "                    \"lora_r\": 4,\n",
        "                    \"use_gradient_checkpointing\": True,\n",
        "                    \"fp16\": False,\n",
        "                    \"bf16\": True\n",
        "                }\n",
        "            else:\n",
        "                return {\n",
        "                    \"batch_size\": 8,\n",
        "                    \"gradient_accumulation_steps\": 8,\n",
        "                    \"max_length\": 256,\n",
        "                    \"lora_r\": 2,\n",
        "                    \"use_gradient_checkpointing\": True,\n",
        "                    \"fp16\": True,\n",
        "                    \"bf16\": False\n",
        "                }\n",
        "\n",
        "        return {\n",
        "            \"batch_size\": 4,\n",
        "            \"gradient_accumulation_steps\": 8,\n",
        "            \"max_length\": 256,\n",
        "            \"lora_r\": 1,\n",
        "            \"use_gradient_checkpointing\": True,\n",
        "            \"fp16\": False,\n",
        "            \"bf16\": False\n",
        "        }\n",
        "\n",
        "# Initialize memory manager and get optimal config\n",
        "memory_manager = GPUMemoryManager()\n",
        "memory_manager.clear_memory()\n",
        "optimal_config = memory_manager.optimize_for_device()\n",
        "\n",
        "print(\"Optimal configuration based on hardware:\")\n",
        "for key, value in optimal_config.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "# Configuration for Chinda-Qwen3-4B model\n",
        "config = {\n",
        "    \"model_name\": \"iapp/chinda-qwen3-4b\",\n",
        "    \"output_dir\": \"/content/drive/MyDrive/V89Technology/chinda-qwen3-4b-medical-finetuned\",\n",
        "    \"max_length\": optimal_config[\"max_length\"],\n",
        "    \"batch_size\": optimal_config[\"batch_size\"],\n",
        "    \"gradient_accumulation_steps\": optimal_config[\"gradient_accumulation_steps\"],\n",
        "    \"learning_rate\": 3e-4,\n",
        "    \"num_epochs\": 3,\n",
        "    \"warmup_ratio\": 0.05,\n",
        "    \"logging_steps\": 5,\n",
        "    \"save_steps\": 5,\n",
        "    \"eval_steps\": 5,\n",
        "    \"lora_r\": optimal_config[\"lora_r\"],\n",
        "    \"lora_alpha\": 8,\n",
        "    \"lora_dropout\": 0.1,\n",
        "    \"use_gradient_checkpointing\": optimal_config[\"use_gradient_checkpointing\"],\n",
        "    \"fp16\": optimal_config[\"fp16\"],\n",
        "    \"bf16\": optimal_config[\"bf16\"],\n",
        "    \"weight_decay\": 0.01,\n",
        "    \"lr_scheduler_type\": \"cosine\",\n",
        "    \"train_ratio\": 0.8,\n",
        "    \"test_ratio\": 0.1,\n",
        "    \"val_ratio\": 0.1,\n",
        "    \"seed\": 42\n",
        "}\n",
        "\n",
        "print(f\"Enhanced configuration ready. Output directory: {config['output_dir']}\")\n",
        "\n",
        "# Advanced quantization configuration\n",
        "def get_advanced_quantization_config():\n",
        "    \"\"\"Get optimized quantization configuration for A100\"\"\"\n",
        "    return BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16 if config[\"bf16\"] else torch.float16,\n",
        "        llm_int8_threshold=6.0,\n",
        "    )\n",
        "\n",
        "# Advanced LoRA configuration for Qwen architecture\n",
        "def get_advanced_lora_config():\n",
        "    \"\"\"Get optimized LoRA configuration for Chinda-Qwen3-4B\"\"\"\n",
        "    return LoraConfig(\n",
        "        task_type=TaskType.CAUSAL_LM,\n",
        "        r=config[\"lora_r\"],\n",
        "        lora_alpha=config[\"lora_alpha\"],\n",
        "        lora_dropout=config[\"lora_dropout\"],\n",
        "        target_modules=[\n",
        "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "            \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "        ],\n",
        "        bias=\"none\",\n",
        "        inference_mode=False,\n",
        "        init_lora_weights=True,\n",
        "        use_rslora=False,\n",
        "    )\n",
        "\n",
        "quantization_config = get_advanced_quantization_config()\n",
        "lora_config = get_advanced_lora_config()\n",
        "\n",
        "print(\"Advanced quantization and LoRA configuration ready\")\n",
        "\n",
        "# Load Thai medical datasets from CSV files - FIXED for Thai columns\n",
        "def load_thai_medical_datasets():\n",
        "    \"\"\"Load Thai medical datasets from CSV files with proper Thai text handling\"\"\"\n",
        "    datasets = {}\n",
        "\n",
        "    csv_files = {\n",
        "        \"medmcqa\": \"medmcqa_thai_132.csv\",\n",
        "        \"mental_health\": \"mental_health_thai_150.csv\",\n",
        "        \"healthcare\": \"healthcare_thai_150.csv\",\n",
        "        \"pubmed\": \"pubmed_thai_150.csv\",\n",
        "        \"medical_qa\": \"medical_qa_thai_150.csv\"\n",
        "    }\n",
        "\n",
        "    for dataset_name, filename in csv_files.items():\n",
        "        try:\n",
        "            file_path = os.path.join(drive_path, filename)\n",
        "            print(f\"Loading {dataset_name} from {file_path}\")\n",
        "\n",
        "            df = pd.read_csv(file_path, encoding='utf-8')\n",
        "            samples = []\n",
        "\n",
        "            if dataset_name == \"medmcqa\":\n",
        "                for _, row in df.iterrows():\n",
        "                    if pd.notna(row['th_question']) and pd.notna(row['th_exp']):\n",
        "                        # Create multiple choice format\n",
        "                        options = []\n",
        "                        if pd.notna(row.get('th_opa')): options.append(f\"‡∏Å) {row['th_opa']}\")\n",
        "                        if pd.notna(row.get('th_opb')): options.append(f\"‡∏Ç) {row['th_opb']}\")\n",
        "                        if pd.notna(row.get('th_opc')): options.append(f\"‡∏Ñ) {row['th_opc']}\")\n",
        "                        if pd.notna(row.get('th_opd')): options.append(f\"‡∏á) {row['th_opd']}\")\n",
        "\n",
        "                        question_with_options = f\"{row['th_question']}\\n\\n\" + \"\\n\".join(options)\n",
        "\n",
        "                        samples.append({\n",
        "                            \"instruction\": \"‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏ó‡∏¢‡πå‡πÅ‡∏•‡∏∞‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•\",\n",
        "                            \"input\": question_with_options,\n",
        "                            \"output\": f\"‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: {row['th_exp']}\",\n",
        "                            \"dataset_type\": \"medical_qa\"\n",
        "                        })\n",
        "\n",
        "            elif dataset_name == \"mental_health\":\n",
        "                for _, row in df.iterrows():\n",
        "                    if pd.notna(row['th_Context']) and pd.notna(row['th_Response']):\n",
        "                        samples.append({\n",
        "                            \"instruction\": \"‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡∏î‡πâ‡∏≤‡∏ô‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û‡∏à‡∏¥‡∏ï\",\n",
        "                            \"input\": row['th_Context'],\n",
        "                            \"output\": row['th_Response'],\n",
        "                            \"dataset_type\": \"mental_health\"\n",
        "                        })\n",
        "\n",
        "            elif dataset_name == \"healthcare\":\n",
        "                for _, row in df.iterrows():\n",
        "                    if pd.notna(row['th_instruction']) and pd.notna(row['th_output']):\n",
        "                        samples.append({\n",
        "                            \"instruction\": row['th_instruction'],\n",
        "                            \"input\": row.get('th_input', '') if pd.notna(row.get('th_input')) else '',\n",
        "                            \"output\": row['th_output'],\n",
        "                            \"dataset_type\": \"healthcare\"\n",
        "                        })\n",
        "\n",
        "            elif dataset_name == \"pubmed\":\n",
        "                for _, row in df.iterrows():\n",
        "                    if pd.notna(row['th_input']) and pd.notna(row['th_output']):\n",
        "                        instruction = row.get('th_instruction', '‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏ó‡∏¢‡πå')\n",
        "                        if pd.isna(instruction):\n",
        "                            instruction = '‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏ó‡∏¢‡πå'\n",
        "\n",
        "                        samples.append({\n",
        "                            \"instruction\": instruction,\n",
        "                            \"input\": row['th_input'],\n",
        "                            \"output\": row['th_output'],\n",
        "                            \"dataset_type\": \"pubmed\"\n",
        "                        })\n",
        "\n",
        "            elif dataset_name == \"medical_qa\":\n",
        "                for _, row in df.iterrows():\n",
        "                    if pd.notna(row['th_instruction']) and pd.notna(row['th_output']):\n",
        "                        samples.append({\n",
        "                            \"instruction\": row['th_instruction'],\n",
        "                            \"input\": row.get('th_input', '') if pd.notna(row.get('th_input')) else '',\n",
        "                            \"output\": row['th_output'],\n",
        "                            \"dataset_type\": \"medical_qa\"\n",
        "                        })\n",
        "\n",
        "            datasets[dataset_name] = samples\n",
        "            print(f\"{dataset_name}: {len(samples)} valid samples loaded\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {dataset_name}: {e}\")\n",
        "            datasets[dataset_name] = []\n",
        "\n",
        "    return datasets\n",
        "\n",
        "# Load datasets\n",
        "print(\"Loading Thai medical datasets from CSV files...\")\n",
        "medical_datasets = load_thai_medical_datasets()\n",
        "\n",
        "# Combine all samples\n",
        "all_samples = []\n",
        "for dataset_name, samples in medical_datasets.items():\n",
        "    all_samples.extend(samples)\n",
        "    print(f\"{dataset_name}: {len(samples)} samples\")\n",
        "\n",
        "print(f\"Total samples loaded: {len(all_samples)}\")\n",
        "\n",
        "# Enhanced train/validation/test split - FIXED split ratios\n",
        "def create_balanced_split(samples, train_ratio=0.8, test_ratio=0.1, val_ratio=0.1, seed=42):\n",
        "    \"\"\"Create balanced train/validation/test split with correct ratios\"\"\"\n",
        "    import random\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Shuffle all samples\n",
        "    random.shuffle(samples)\n",
        "\n",
        "    total_samples = len(samples)\n",
        "    train_end = int(total_samples * train_ratio)\n",
        "    test_end = train_end + int(total_samples * test_ratio)\n",
        "\n",
        "    train_samples = samples[:train_end]\n",
        "    test_samples = samples[train_end:test_end]\n",
        "    val_samples = samples[test_end:]\n",
        "\n",
        "    return train_samples, val_samples, test_samples\n",
        "\n",
        "train_samples, val_samples, test_samples = create_balanced_split(\n",
        "    all_samples,\n",
        "    train_ratio=config[\"train_ratio\"],\n",
        "    test_ratio=config[\"test_ratio\"],\n",
        "    val_ratio=config[\"val_ratio\"],\n",
        "    seed=config[\"seed\"]\n",
        ")\n",
        "\n",
        "print(f\"Training samples: {len(train_samples)}\")\n",
        "print(f\"Validation samples: {len(val_samples)}\")\n",
        "print(f\"Test samples: {len(test_samples)}\")\n",
        "\n",
        "# Validate that we have sufficient samples\n",
        "if len(train_samples) < 10:\n",
        "    raise ValueError(\"Insufficient training samples. Need at least 10 samples.\")\n",
        "if len(val_samples) < 2:\n",
        "    raise ValueError(\"Insufficient validation samples. Need at least 2 samples.\")\n",
        "if len(test_samples) < 2:\n",
        "    raise ValueError(\"Insufficient test samples. Need at least 2 samples.\")\n",
        "\n",
        "# Enhanced tokenizer and model loading with proper encoding - FIXED attention mask issue\n",
        "print(\"Loading Chinda-Qwen3-4B model and tokenizer with optimizations...\")\n",
        "\n",
        "# Load tokenizer with advanced settings - FIXED pad token issue\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    config[\"model_name\"],\n",
        "    trust_remote_code=True,\n",
        "    padding_side='right',\n",
        "    use_fast=True,\n",
        ")\n",
        "\n",
        "# FIXED: Properly configure pad token to avoid attention mask issues\n",
        "if tokenizer.pad_token is None:\n",
        "    if tokenizer.unk_token is not None:\n",
        "        tokenizer.pad_token = tokenizer.unk_token\n",
        "        tokenizer.pad_token_id = tokenizer.unk_token_id\n",
        "    else:\n",
        "        tokenizer.add_special_tokens({'pad_token': '<|pad|>'})\n",
        "\n",
        "# Ensure we have the required special tokens\n",
        "if tokenizer.eos_token is None:\n",
        "    tokenizer.add_special_tokens({'eos_token': '<|endoftext|>'})\n",
        "\n",
        "print(f\"Tokenizer configured - Pad token: {tokenizer.pad_token}, EOS token: {tokenizer.eos_token}\")\n",
        "\n",
        "print(\"Loading model with advanced optimizations...\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    config[\"model_name\"],\n",
        "    quantization_config=quantization_config,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16 if config[\"bf16\"] else torch.float16,\n",
        "    use_cache=False,\n",
        "    low_cpu_mem_usage=True,\n",
        ")\n",
        "\n",
        "# Enable gradient checkpointing for memory efficiency\n",
        "if config[\"use_gradient_checkpointing\"]:\n",
        "    model.gradient_checkpointing_enable()\n",
        "\n",
        "# Prepare model for k-bit training\n",
        "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=config[\"use_gradient_checkpointing\"])\n",
        "\n",
        "# Apply LoRA configuration\n",
        "print(\"Applying LoRA configuration...\")\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Print trainable parameters\n",
        "def print_trainable_parameters(model):\n",
        "    \"\"\"Print the number of trainable parameters in the model.\"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(f\"Trainable params: {trainable_params:,} || All params: {all_param:,} || Trainable%: {100 * trainable_params / all_param:.2f}\")\n",
        "\n",
        "print_trainable_parameters(model)\n",
        "\n",
        "# Enhanced data preprocessing with proper Thai text handling - FIXED tensor creation issue\n",
        "def format_training_sample(sample):\n",
        "    \"\"\"Format sample for training with proper Thai text encoding\"\"\"\n",
        "    instruction = str(sample.get(\"instruction\", \"\")).strip()\n",
        "    input_text = str(sample.get(\"input\", \"\")).strip()\n",
        "    output_text = str(sample.get(\"output\", \"\")).strip()\n",
        "\n",
        "    # Create Thai medical conversation format for Qwen\n",
        "    if input_text and input_text != \"\":\n",
        "        prompt = f\"<|im_start|>system\\n‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏ó‡∏¢‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢<|im_end|>\\n<|im_start|>user\\n{instruction}\\n\\n{input_text}<|im_end|>\\n<|im_start|>assistant\\n{output_text}<|im_end|>\"\n",
        "    else:\n",
        "        prompt = f\"<|im_start|>system\\n‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏ó‡∏¢‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢<|im_end|>\\n<|im_start|>user\\n{instruction}<|im_end|>\\n<|im_start|>assistant\\n{output_text}<|im_end|>\"\n",
        "\n",
        "    return prompt\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    \"\"\"Enhanced preprocessing with proper tokenization and padding - FIXED tensor creation\"\"\"\n",
        "    # Format all samples\n",
        "    formatted_texts = []\n",
        "    for i in range(len(examples[\"instruction\"])):\n",
        "        sample = {\n",
        "            \"instruction\": examples[\"instruction\"][i],\n",
        "            \"input\": examples.get(\"input\", [\"\"] * len(examples[\"instruction\"]))[i],\n",
        "            \"output\": examples[\"output\"][i]\n",
        "        }\n",
        "        formatted_texts.append(format_training_sample(sample))\n",
        "\n",
        "    # Tokenize with proper settings and explicit padding - FIXED truncation\n",
        "    tokenized = tokenizer(\n",
        "        formatted_texts,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=config[\"max_length\"],\n",
        "        return_tensors=None,\n",
        "        add_special_tokens=False  # We already added them in format_training_sample\n",
        "    )\n",
        "\n",
        "    # FIXED: Proper attention mask handling\n",
        "    attention_mask = []\n",
        "    labels = []\n",
        "\n",
        "    for i, input_ids in enumerate(tokenized[\"input_ids\"]):\n",
        "        # Create attention mask (1 for real tokens, 0 for padding)\n",
        "        mask = [1 if token_id != tokenizer.pad_token_id else 0 for token_id in input_ids]\n",
        "        attention_mask.append(mask)\n",
        "\n",
        "        # Create labels (copy of input_ids, with -100 for padding tokens)\n",
        "        label = [token_id if token_id != tokenizer.pad_token_id else -100 for token_id in input_ids]\n",
        "        labels.append(label)\n",
        "\n",
        "    tokenized[\"attention_mask\"] = attention_mask\n",
        "    tokenized[\"labels\"] = labels\n",
        "\n",
        "    return tokenized\n",
        "\n",
        "# Convert samples to dataset format\n",
        "def samples_to_dataset_dict(samples):\n",
        "    \"\"\"Convert samples to dataset format with validation\"\"\"\n",
        "    dataset_dict = {\n",
        "        \"instruction\": [],\n",
        "        \"input\": [],\n",
        "        \"output\": [],\n",
        "        \"dataset_type\": []\n",
        "    }\n",
        "\n",
        "    for sample in samples:\n",
        "        if isinstance(sample, dict):\n",
        "            dataset_dict[\"instruction\"].append(sample.get(\"instruction\", \"\"))\n",
        "            dataset_dict[\"input\"].append(sample.get(\"input\", \"\"))\n",
        "            dataset_dict[\"output\"].append(sample.get(\"output\", \"\"))\n",
        "            dataset_dict[\"dataset_type\"].append(sample.get(\"dataset_type\", \"general\"))\n",
        "\n",
        "    return dataset_dict\n",
        "\n",
        "print(\"Creating datasets...\")\n",
        "train_dataset_dict = samples_to_dataset_dict(train_samples)\n",
        "val_dataset_dict = samples_to_dataset_dict(val_samples)\n",
        "test_dataset_dict = samples_to_dataset_dict(test_samples)\n",
        "\n",
        "# Create Dataset objects\n",
        "train_dataset = Dataset.from_dict(train_dataset_dict)\n",
        "val_dataset = Dataset.from_dict(val_dataset_dict)\n",
        "test_dataset = Dataset.from_dict(test_dataset_dict)\n",
        "\n",
        "# Apply preprocessing\n",
        "print(\"Applying preprocessing...\")\n",
        "train_dataset = train_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=train_dataset.column_names,\n",
        "    desc=\"Preprocessing training data\"\n",
        ")\n",
        "\n",
        "val_dataset = val_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=val_dataset.column_names,\n",
        "    desc=\"Preprocessing validation data\"\n",
        ")\n",
        "\n",
        "print(f\"Training dataset size: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")\n",
        "\n",
        "# FIXED: Enhanced data collator with proper padding\n",
        "class CustomDataCollator:\n",
        "    \"\"\"Custom data collator to handle Thai text properly\"\"\"\n",
        "\n",
        "    def __init__(self, tokenizer, pad_to_multiple_of=8):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.pad_to_multiple_of = pad_to_multiple_of\n",
        "\n",
        "    def __call__(self, features):\n",
        "        batch = {}\n",
        "\n",
        "        # Get max length in this batch\n",
        "        max_len = max(len(f[\"input_ids\"]) for f in features)\n",
        "        if self.pad_to_multiple_of:\n",
        "            max_len = ((max_len + self.pad_to_multiple_of - 1) // self.pad_to_multiple_of) * self.pad_to_multiple_of\n",
        "\n",
        "        # Pad all features to max length\n",
        "        batch_input_ids = []\n",
        "        batch_attention_mask = []\n",
        "        batch_labels = []\n",
        "\n",
        "        for f in features:\n",
        "            input_ids = f[\"input_ids\"]\n",
        "            attention_mask = f[\"attention_mask\"]\n",
        "            labels = f[\"labels\"]\n",
        "\n",
        "            # Pad to max_len\n",
        "            padding_length = max_len - len(input_ids)\n",
        "\n",
        "            input_ids = input_ids + [self.tokenizer.pad_token_id] * padding_length\n",
        "            attention_mask = attention_mask + [0] * padding_length\n",
        "            labels = labels + [-100] * padding_length\n",
        "\n",
        "            batch_input_ids.append(input_ids)\n",
        "            batch_attention_mask.append(attention_mask)\n",
        "            batch_labels.append(labels)\n",
        "\n",
        "        batch[\"input_ids\"] = torch.tensor(batch_input_ids, dtype=torch.long)\n",
        "        batch[\"attention_mask\"] = torch.tensor(batch_attention_mask, dtype=torch.long)\n",
        "        batch[\"labels\"] = torch.tensor(batch_labels, dtype=torch.long)\n",
        "\n",
        "        return batch\n",
        "\n",
        "data_collator = CustomDataCollator(tokenizer=tokenizer, pad_to_multiple_of=8)\n",
        "\n",
        "# Advanced evaluation metrics - FIXED BLEU score calculation\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Compute advanced metrics for medical fine-tuning evaluation - FIXED BLEU score\"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "\n",
        "    # Ensure predictions and labels are numpy arrays\n",
        "    if isinstance(predictions, torch.Tensor):\n",
        "        predictions = predictions.cpu().numpy()\n",
        "    if isinstance(labels, torch.Tensor):\n",
        "        labels = labels.cpu().numpy()\n",
        "\n",
        "    # Calculate perplexity\n",
        "    predictions_flat = predictions.reshape(-1, predictions.shape[-1])\n",
        "    labels_flat = labels.reshape(-1)\n",
        "\n",
        "    # Filter out ignored tokens (typically -100)\n",
        "    mask = labels_flat != -100\n",
        "    if mask.sum() == 0:\n",
        "        return {\"perplexity\": float('inf'), \"eval_loss\": float('inf'), \"bleu\": 0.0}\n",
        "\n",
        "    predictions_filtered = predictions_flat[mask]\n",
        "    labels_filtered = labels_flat[mask]\n",
        "\n",
        "    # Calculate cross-entropy loss\n",
        "    log_probs = F.log_softmax(torch.tensor(predictions_filtered, dtype=torch.float32), dim=-1)\n",
        "    nll_loss = F.nll_loss(log_probs, torch.tensor(labels_filtered), reduction='mean')\n",
        "    perplexity = torch.exp(nll_loss).item()\n",
        "\n",
        "    # FIXED: Calculate BLEU score (0 <= BLEU <= 1)\n",
        "    try:\n",
        "        # Get predicted token ids\n",
        "        pred_token_ids = np.argmax(predictions_filtered, axis=-1)\n",
        "\n",
        "        # Decode predictions and references\n",
        "        pred_texts = []\n",
        "        ref_texts = []\n",
        "\n",
        "        # Process in small batches to avoid memory issues\n",
        "        batch_size = 16\n",
        "        for i in range(0, len(pred_token_ids), batch_size):\n",
        "            batch_pred = pred_token_ids[i:i+batch_size]\n",
        "            batch_ref = labels_filtered[i:i+batch_size]\n",
        "\n",
        "            try:\n",
        "                pred_text = tokenizer.decode(batch_pred, skip_special_tokens=True).strip()\n",
        "                ref_text = tokenizer.decode(batch_ref, skip_special_tokens=True).strip()\n",
        "\n",
        "                if pred_text and ref_text:\n",
        "                    pred_texts.append(pred_text)\n",
        "                    ref_texts.append([ref_text])  # BLEU expects list of references\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        if pred_texts and ref_texts:\n",
        "            from sacrebleu import corpus_bleu\n",
        "            bleu_score = corpus_bleu(pred_texts, ref_texts).score / 100.0  # Convert to 0-1 range\n",
        "            bleu_score = min(max(bleu_score, 0.0), 1.0)  # Ensure 0 <= BLEU <= 1\n",
        "        else:\n",
        "            bleu_score = 0.0\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"BLEU calculation error: {e}\")\n",
        "        bleu_score = 0.0\n",
        "\n",
        "    return {\n",
        "        \"perplexity\": perplexity,\n",
        "        \"eval_loss\": nll_loss.item(),\n",
        "        \"bleu\": bleu_score\n",
        "    }\n",
        "\n",
        "# FIXED: Custom Trainer to handle accelerate unwrap_model issue\n",
        "class CustomTrainer(Trainer):\n",
        "    \"\"\"Custom trainer to fix accelerate unwrap_model compatibility issue\"\"\"\n",
        "\n",
        "    def _wrap_model(self, model, training=True, dataloader=None):\n",
        "        \"\"\"Override _wrap_model to fix unwrap_model compatibility\"\"\"\n",
        "        if self.args.use_ipex:\n",
        "            dtype = torch.bfloat16 if self.use_cpu_amp else torch.float32\n",
        "            model = self.accelerator.prepare(model)\n",
        "        else:\n",
        "            model = self.accelerator.prepare(model)\n",
        "\n",
        "        # Fix for accelerate compatibility - avoid keep_torch_compile parameter\n",
        "        if hasattr(self.accelerator, '_models'):\n",
        "            if model not in self.accelerator._models:\n",
        "                return model\n",
        "\n",
        "        return model\n",
        "\n",
        "# Advanced training arguments with A100 optimizations - FIXED accelerate compatibility\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=config[\"output_dir\"],\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=config[\"num_epochs\"],\n",
        "    per_device_train_batch_size=config[\"batch_size\"],\n",
        "    per_device_eval_batch_size=config[\"batch_size\"],\n",
        "    gradient_accumulation_steps=config[\"gradient_accumulation_steps\"],\n",
        "    gradient_checkpointing=config[\"use_gradient_checkpointing\"],\n",
        "    learning_rate=config[\"learning_rate\"],\n",
        "    weight_decay=config[\"weight_decay\"],\n",
        "    lr_scheduler_type=config[\"lr_scheduler_type\"],\n",
        "    warmup_ratio=config[\"warmup_ratio\"],\n",
        "    logging_steps=config[\"logging_steps\"],\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=config[\"eval_steps\"],\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=config[\"save_steps\"],\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    fp16=config[\"fp16\"],\n",
        "    bf16=config[\"bf16\"],\n",
        "    dataloader_pin_memory=True,\n",
        "    dataloader_num_workers=0,  # Reduced for stability\n",
        "    remove_unused_columns=False,\n",
        "    report_to=None,\n",
        "    run_name=\"chinda_qwen3_4b_medical_v4.0\",\n",
        "    seed=config[\"seed\"],\n",
        "    data_seed=config[\"seed\"],\n",
        "    group_by_length=False,\n",
        "    length_column_name=\"input_ids\",\n",
        "    ddp_find_unused_parameters=False,\n",
        "    dataloader_drop_last=False,\n",
        "    eval_accumulation_steps=1,\n",
        "    prediction_loss_only=False,\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "# Custom callback for enhanced monitoring\n",
        "class EnhancedTrainingCallback(TrainerCallback):\n",
        "    \"\"\"Enhanced callback for monitoring training progress and GPU memory\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.training_started = False\n",
        "\n",
        "    def on_train_begin(self, args, state, control, **kwargs):\n",
        "        \"\"\"Called at the beginning of training\"\"\"\n",
        "        self.training_started = True\n",
        "        print(\"üöÄ Enhanced Chinda-Qwen3-4B Medical Fine-tuning Started!\")\n",
        "        print(f\"üìä Training Configuration:\")\n",
        "        print(f\"   ‚Ä¢ Total epochs: {args.num_train_epochs}\")\n",
        "        print(f\"   ‚Ä¢ Batch size: {args.per_device_train_batch_size}\")\n",
        "        print(f\"   ‚Ä¢ Gradient accumulation: {args.gradient_accumulation_steps}\")\n",
        "        print(f\"   ‚Ä¢ Learning rate: {args.learning_rate}\")\n",
        "        print(f\"   ‚Ä¢ Effective batch size: {args.per_device_train_batch_size * args.gradient_accumulation_steps}\")\n",
        "\n",
        "        # Display GPU memory info\n",
        "        memory_info = memory_manager.get_memory_info()\n",
        "        if \"error\" not in memory_info:\n",
        "            print(f\"   ‚Ä¢ GPU Memory: {memory_info['allocated_gb']:.1f}GB allocated, {memory_info['free_gb']:.1f}GB free\")\n",
        "\n",
        "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
        "        \"\"\"Called at the beginning of each epoch\"\"\"\n",
        "        current_epoch = int(state.epoch) + 1 if state.epoch is not None else 1\n",
        "        print(f\"\\nüìà Starting Epoch {current_epoch}/{args.num_train_epochs}\")\n",
        "\n",
        "        # Clear memory at epoch start\n",
        "        memory_manager.clear_memory()\n",
        "\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        \"\"\"Called when logging\"\"\"\n",
        "        if logs and self.training_started:\n",
        "            step = logs.get('step', state.global_step)\n",
        "\n",
        "            # Training metrics\n",
        "            if 'loss' in logs:\n",
        "                print(f\"Step {step}: Loss = {logs['loss']:.4f}\")\n",
        "\n",
        "            # Evaluation metrics\n",
        "            if 'eval_loss' in logs:\n",
        "                eval_loss = logs['eval_loss']\n",
        "                perplexity = logs.get('eval_perplexity', math.exp(eval_loss) if eval_loss < 10 else float('inf'))\n",
        "                bleu = logs.get('eval_bleu', 0.0)\n",
        "                print(f\"üìä Evaluation at Step {step}:\")\n",
        "                print(f\"   ‚Ä¢ Eval Loss: {eval_loss:.4f}\")\n",
        "                print(f\"   ‚Ä¢ Perplexity: {perplexity:.2f}\")\n",
        "                print(f\"   ‚Ä¢ BLEU Score: {bleu:.4f}\")\n",
        "\n",
        "                # GPU memory status\n",
        "                memory_info = memory_manager.get_memory_info()\n",
        "                if \"error\" not in memory_info:\n",
        "                    print(f\"   ‚Ä¢ GPU Memory: {memory_info['allocated_gb']:.1f}GB used\")\n",
        "\n",
        "    def on_epoch_end(self, args, state, control, **kwargs):\n",
        "        \"\"\"Called at the end of each epoch\"\"\"\n",
        "        current_epoch = int(state.epoch) if state.epoch is not None else 1\n",
        "        print(f\"‚úÖ Completed Epoch {current_epoch}\")\n",
        "\n",
        "        # Memory cleanup\n",
        "        memory_manager.clear_memory()\n",
        "\n",
        "    def on_train_end(self, args, state, control, **kwargs):\n",
        "        \"\"\"Called at the end of training\"\"\"\n",
        "        print(\"üéâ Enhanced Chinda-Qwen3-4B Medical Fine-tuning Completed!\")\n",
        "        print(f\"üìä Final Training Statistics:\")\n",
        "        print(f\"   ‚Ä¢ Total steps completed: {state.global_step}\")\n",
        "        print(f\"   ‚Ä¢ Best model saved at: {args.output_dir}\")\n",
        "\n",
        "        # Final memory cleanup\n",
        "        memory_manager.clear_memory()\n",
        "\n",
        "# FIXED: Enhanced trainer with optimized settings\n",
        "trainer = CustomTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[\n",
        "        EnhancedTrainingCallback(),\n",
        "        EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.01)\n",
        "    ],\n",
        ")\n",
        "\n",
        "# Pre-training validation - FIXED training setup validation\n",
        "def validate_training_setup():\n",
        "    \"\"\"Validate training setup before starting\"\"\"\n",
        "    try:\n",
        "        print(\"üîç Pre-training System Checks:\")\n",
        "\n",
        "        # Check model\n",
        "        if model is None:\n",
        "            raise ValueError(\"Model not loaded\")\n",
        "        print(f\"‚úÖ Model loaded: {model.__class__.__name__}\")\n",
        "\n",
        "        # Check LoRA\n",
        "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "        if trainable_params == 0:\n",
        "            raise ValueError(\"No trainable parameters found\")\n",
        "        print(f\"‚úÖ LoRA applied: {trainable_params:,} trainable parameters\")\n",
        "\n",
        "        # Check datasets\n",
        "        if len(train_dataset) == 0:\n",
        "            raise ValueError(\"No training samples\")\n",
        "        if len(val_dataset) == 0:\n",
        "            raise ValueError(\"No validation samples\")\n",
        "        print(f\"‚úÖ Training samples: {len(train_dataset)}\")\n",
        "        print(f\"‚úÖ Validation samples: {len(val_dataset)}\")\n",
        "\n",
        "        # Check tokenizer\n",
        "        if tokenizer.pad_token is None:\n",
        "            raise ValueError(\"Tokenizer pad_token not set\")\n",
        "        print(f\"‚úÖ Tokenizer configured properly\")\n",
        "\n",
        "        # Test data collator with a small batch\n",
        "        test_batch = [train_dataset[0], train_dataset[1] if len(train_dataset) > 1 else train_dataset[0]]\n",
        "        collated_batch = data_collator(test_batch)\n",
        "\n",
        "        if 'input_ids' not in collated_batch or 'attention_mask' not in collated_batch:\n",
        "            raise ValueError(\"Data collator failed\")\n",
        "        print(f\"‚úÖ Data collator working properly\")\n",
        "\n",
        "        # Test model forward pass\n",
        "        test_input = {k: v[:1] for k, v in collated_batch.items()}  # Take first sample\n",
        "        test_input = {k: v.to(model.device) for k, v in test_input.items()}\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**test_input)\n",
        "            if outputs.loss is None:\n",
        "                raise ValueError(\"Model forward pass failed\")\n",
        "        print(f\"‚úÖ Model forward pass successful\")\n",
        "\n",
        "        model.train()  # Switch back to training mode\n",
        "\n",
        "        print(\"‚úÖ All pre-training checks passed!\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Training setup validation failed: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "# Run validation\n",
        "if not validate_training_setup():\n",
        "    raise RuntimeError(\"Training setup validation failed. Aborting training.\")\n",
        "\n",
        "# Display memory usage before training\n",
        "memory_info = memory_manager.get_memory_info()\n",
        "if \"error\" not in memory_info:\n",
        "    print(f\"‚úÖ GPU Memory: {memory_info['allocated_gb']:.1f}GB allocated, {memory_info['free_gb']:.1f}GB available\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è GPU not available - running on CPU\")\n",
        "\n",
        "# Enhanced training execution with error handling\n",
        "try:\n",
        "    print(\"\\nüöÄ Starting Enhanced Chinda-Qwen3-4B Medical Fine-tuning...\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Clear memory before training\n",
        "    memory_manager.clear_memory()\n",
        "\n",
        "    # Start training\n",
        "    trainer.train()\n",
        "\n",
        "    # Save the final model\n",
        "    print(\"\\nüíæ Saving final model...\")\n",
        "    final_model_path = os.path.join(config[\"output_dir\"], \"final_model\")\n",
        "    trainer.save_model(final_model_path)\n",
        "    tokenizer.save_pretrained(final_model_path)\n",
        "\n",
        "    print(f\"‚úÖ Model saved to: {final_model_path}\")\n",
        "\n",
        "    # Save training configuration\n",
        "    config_path = os.path.join(final_model_path, \"training_config.json\")\n",
        "    with open(config_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"‚úÖ Training configuration saved to: {config_path}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Training error: {str(e)}\")\n",
        "    print(\"üí° Troubleshooting suggestions:\")\n",
        "    print(\"   ‚Ä¢ Reduce batch_size in config\")\n",
        "    print(\"   ‚Ä¢ Enable gradient_checkpointing\")\n",
        "    print(\"   ‚Ä¢ Reduce max_length\")\n",
        "    print(\"   ‚Ä¢ Check GPU memory availability\")\n",
        "    raise\n",
        "\n",
        "# Post-training testing functions - FIXED perplexity calculation\n",
        "def calculate_perplexity(model, tokenizer, test_samples):\n",
        "    \"\"\"Calculate perplexity on test dataset - FIXED calculation\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_tokens = 0\n",
        "\n",
        "    print(\"\\nüìä Calculating Perplexity on Test Set...\")\n",
        "\n",
        "    # Process test samples\n",
        "    test_losses = []\n",
        "    for i, sample in enumerate(test_samples[:20]):  # Limit to 20 samples for efficiency\n",
        "        try:\n",
        "            # Format sample\n",
        "            formatted_text = format_training_sample(sample)\n",
        "\n",
        "            # Tokenize\n",
        "            inputs = tokenizer(\n",
        "                formatted_text,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                max_length=config[\"max_length\"],\n",
        "                padding=True\n",
        "            )\n",
        "\n",
        "            # Move to device\n",
        "            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "            inputs[\"labels\"] = inputs[\"input_ids\"].clone()\n",
        "\n",
        "            # Forward pass\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "                loss = outputs.loss\n",
        "\n",
        "            if torch.isfinite(loss):\n",
        "                test_losses.append(loss.item())\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing sample {i}: {e}\")\n",
        "            continue\n",
        "\n",
        "    if test_losses:\n",
        "        avg_loss = np.mean(test_losses)\n",
        "        perplexity = math.exp(avg_loss)\n",
        "        print(f\"‚úÖ Test Perplexity: {perplexity:.2f}\")\n",
        "        return perplexity\n",
        "    else:\n",
        "        print(\"‚ùå Failed to calculate perplexity\")\n",
        "        return float('inf')\n",
        "\n",
        "def test_model_inference(model, tokenizer, test_samples, num_examples=3):\n",
        "    \"\"\"Test model inference on sample examples\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    generation_config = GenerationConfig(\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        max_new_tokens=256,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        repetition_penalty=1.1,\n",
        "    )\n",
        "\n",
        "    print(\"\\nüß™ Model Inference Testing:\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    for i in range(min(num_examples, len(test_samples))):\n",
        "        sample = test_samples[i]\n",
        "        instruction = sample.get(\"instruction\", \"\")\n",
        "        input_text = sample.get(\"input\", \"\")\n",
        "        expected_output = sample.get(\"output\", \"\")\n",
        "\n",
        "        # Create prompt for generation (without the expected output)\n",
        "        if input_text and input_text.strip():\n",
        "            prompt = f\"<|im_start|>system\\n‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏ó‡∏¢‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢<|im_end|>\\n<|im_start|>user\\n{instruction}\\n\\n{input_text}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
        "        else:\n",
        "            prompt = f\"<|im_start|>system\\n‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏ó‡∏¢‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢<|im_end|>\\n<|im_start|>user\\n{instruction}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
        "\n",
        "        print(f\"\\nüîç Example {i+1}:\")\n",
        "        print(f\"Instruction: {instruction}\")\n",
        "        if input_text:\n",
        "            print(f\"Input: {input_text}\")\n",
        "\n",
        "        try:\n",
        "            inputs = tokenizer(\n",
        "                prompt,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                max_length=config[\"max_length\"] // 2,\n",
        "                padding=True\n",
        "            ).to(model.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(\n",
        "                    **inputs,\n",
        "                    generation_config=generation_config,\n",
        "                    use_cache=True\n",
        "                )\n",
        "\n",
        "            response = tokenizer.decode(\n",
        "                outputs[0][inputs['input_ids'].shape[1]:],\n",
        "                skip_special_tokens=True\n",
        "            ).strip()\n",
        "\n",
        "            print(f\"Generated Response: {response}\")\n",
        "            print(f\"Expected Response: {expected_output}\")\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Generation error: {str(e)}\")\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "# Run post-training tests\n",
        "try:\n",
        "    print(\"\\nüß™ Running post-training tests...\")\n",
        "\n",
        "    # Test perplexity measurement\n",
        "    perplexity = calculate_perplexity(trainer.model, tokenizer, test_samples)\n",
        "    print(f\"üìä Perplexity Measurement Output: {perplexity:.2f}\")\n",
        "\n",
        "    # Test inference for 3 examples\n",
        "    print(\"\\nüîç Testing Model Inference (3 examples):\")\n",
        "    test_model_inference(trainer.model, tokenizer, test_samples, num_examples=3)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Post-training testing failed: {str(e)}\")\n",
        "\n",
        "# Load and test the fine-tuned model - ADDITIONAL TEST\n",
        "def load_and_test_finetuned_model():\n",
        "    \"\"\"Load the fine-tuned model and test it\"\"\"\n",
        "    try:\n",
        "        print(\"\\nüîÑ Loading fine-tuned model for testing...\")\n",
        "\n",
        "        final_model_path = os.path.join(config[\"output_dir\"], \"final_model\")\n",
        "\n",
        "        # Load the fine-tuned model\n",
        "        test_model = AutoModelForCausalLM.from_pretrained(\n",
        "            final_model_path,\n",
        "            torch_dtype=torch.bfloat16 if config[\"bf16\"] else torch.float16,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        test_tokenizer = AutoTokenizer.from_pretrained(\n",
        "            final_model_path,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        print(\"‚úÖ Loaded fine-tuned model for testing\")\n",
        "\n",
        "        # Test with a sample medical question\n",
        "        test_question = \"‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏Å‡∏©‡∏≤‡πÇ‡∏£‡∏Ñ‡πÄ‡∏ö‡∏≤‡∏´‡∏ß‡∏≤‡∏ô\"\n",
        "\n",
        "        prompt = f\"<|im_start|>system\\n‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏ó‡∏¢‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢<|im_end|>\\n<|im_start|>user\\n{test_question}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
        "\n",
        "        inputs = test_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(test_model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = test_model.generate(\n",
        "                **inputs,\n",
        "                do_sample=True,\n",
        "                temperature=0.7,\n",
        "                max_new_tokens=200,\n",
        "                pad_token_id=test_tokenizer.pad_token_id,\n",
        "                eos_token_id=test_tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        response = test_tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
        "\n",
        "        print(f\"‚úÖ Fine-tuned model test successful!\")\n",
        "        print(f\"Question: {test_question}\")\n",
        "        print(f\"Response: {response.strip()}\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Fine-tuned model test failed: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "# Test the fine-tuned model\n",
        "load_and_test_finetuned_model()\n",
        "\n",
        "# Final cleanup and summary\n",
        "print(\"\\nüéä Enhanced Chinda-Qwen3-4B Medical Fine-tuning Summary:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"‚úÖ Model: {config['model_name']}\")\n",
        "print(f\"‚úÖ Training samples: {len(train_samples)}\")\n",
        "print(f\"‚úÖ Validation samples: {len(val_samples)}\")\n",
        "print(f\"‚úÖ Test samples: {len(test_samples)}\")\n",
        "print(f\"‚úÖ Train/Test/Val split: {config['train_ratio']}/{config['test_ratio']}/{config['val_ratio']}\")\n",
        "print(f\"‚úÖ Epochs completed: {config['num_epochs']}\")\n",
        "print(f\"‚úÖ LoRA rank: {config['lora_r']}\")\n",
        "print(f\"‚úÖ Learning rate: {config['learning_rate']}\")\n",
        "print(f\"‚úÖ Final model saved to: {os.path.join(config['output_dir'], 'final_model')}\")\n",
        "\n",
        "# Final memory cleanup\n",
        "memory_manager.clear_memory()\n",
        "print(f\"‚úÖ Memory cleanup completed\")\n",
        "\n",
        "print(\"\\nüöÄ Enhanced Chinda-Qwen3-4B Medical Fine-tuning Process Completed Successfully!\")\n",
        "print(\"üìö The model is now ready for Thai medical applications.\")\n",
        "print(\"üí° Remember to test thoroughly before production use.\")\n",
        "\n",
        "# Additional metrics and version fixes summary\n",
        "print(\"\\nüîß Version 4.0 Fixes Applied:\")\n",
        "print(\"‚úÖ Fixed: Accelerator.unwrap_model() compatibility issue\")\n",
        "print(\"‚úÖ Fixed: Dataset processing with proper Thai text handling\")\n",
        "print(\"‚úÖ Fixed: Attention mask configuration to avoid inference warnings\")\n",
        "print(\"‚úÖ Fixed: BLEU score calculation (0 ‚â§ BLEU ‚â§ 1)\")\n",
        "print(\"‚úÖ Fixed: Proper train/test/val split (80/10/10)\")\n",
        "print(\"‚úÖ Fixed: Tensor creation with proper padding and truncation\")\n",
        "print(\"‚úÖ Added: Perplexity measurement and output\")\n",
        "print(\"‚úÖ Added: Model inference testing with 3 examples\")\n",
        "print(\"‚úÖ Added: Comprehensive error handling and validation\")"
      ],
      "metadata": {
        "id": "xuKUfegPZ38Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}