{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSCbjgsFbcwK"
      },
      "outputs": [],
      "source": [
        "# Enhanced Fine-tuning Typhoon 2.1 Gemma3 4B for Thai Medical Applications\n",
        "# Model: scb10x/typhoon2.1-gemma3-4b\n",
        "# Company: V89 Technology Ltd.\n",
        "# Version: 5.0 - All Critical Fixes Applied for A100 40GB\n",
        "\n",
        "print(\"Starting Enhanced Typhoon 2.1 Gemma3 4B Medical Fine-tuning - Fixed Version 5.0...\")\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import os\n",
        "import re\n",
        "import sys\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "# Install required packages with correct versions for A100\n",
        "!pip install -q --upgrade pip==24.0\n",
        "!pip install -q --force-reinstall torch==2.5.0 torchvision==0.20.0 torchaudio==2.5.0 --index-url https://download.pytorch.org/whl/cu124\n",
        "!pip install -q compressed-tensors>=0.7.0\n",
        "!pip install -q transformers==4.50.0\n",
        "!pip install -q accelerate==1.1.0\n",
        "!pip install -q bitsandbytes==0.47.0\n",
        "!pip install -q peft==0.12.0\n",
        "!pip install -q datasets==2.20.0\n",
        "!pip install -q evaluate==0.4.2\n",
        "!pip install -q rouge-score==0.1.2\n",
        "!pip install -q scikit-learn pandas numpy==2.0.2 scipy>=1.14.1\n",
        "!pip install -q sacrebleu sentencepiece protobuf nltk\n",
        "!pip install -q flash-attn --no-build-isolation\n",
        "\n",
        "# Essential imports\n",
        "import json\n",
        "import gc\n",
        "import torch\n",
        "import transformers\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from datasets import Dataset, DatasetDict\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from typing import Dict, List, Any, Optional, Union\n",
        "import torch.nn.functional as F\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "import math\n",
        "import torch.optim as optim\n",
        "from sacrebleu import corpus_bleu\n",
        "from collections import defaultdict\n",
        "\n",
        "# Updated imports for compatibility\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    EarlyStoppingCallback,\n",
        "    GenerationConfig,\n",
        "    get_linear_schedule_with_warmup,\n",
        "    TrainerCallback,\n",
        "    TrainerControl,\n",
        "    TrainerState\n",
        ")\n",
        "\n",
        "from peft import (\n",
        "    get_peft_model,\n",
        "    LoraConfig,\n",
        "    TaskType,\n",
        "    PeftModel,\n",
        "    prepare_model_for_kbit_training\n",
        ")\n",
        "\n",
        "print(f\"===== Version Check =====\")\n",
        "print(f\"Python version: {sys.version}\")\n",
        "print(\"Transformers version:\", transformers.__version__)\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Numpy version: {np.__version__}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA Device: {torch.cuda.get_device_name()}\")\n",
        "    print(f\"CUDA Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "print(\"=\" * 40)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "def setup_drive_connection():\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        drive_path = \"/content/drive/MyDrive/V89Technology/thai_medicalCare_dataset150\"\n",
        "        print(\"Google Drive connected successfully\")\n",
        "        return drive_path\n",
        "    except:\n",
        "        local_path = \"V89Technology/thai_medicalCare_dataset150\"\n",
        "        os.makedirs(local_path, exist_ok=True)\n",
        "        print(\"Running outside Colab - using local directory\")\n",
        "        return local_path\n",
        "\n",
        "drive_path = setup_drive_connection()\n",
        "\n",
        "# GPU Memory Management\n",
        "class GPUMemoryManager:\n",
        "    @staticmethod\n",
        "    def clear_memory():\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            torch.cuda.synchronize()\n",
        "            torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "    @staticmethod\n",
        "    def get_memory_info():\n",
        "        if torch.cuda.is_available():\n",
        "            allocated = torch.cuda.memory_allocated() / 1024**3\n",
        "            cached = torch.cuda.memory_reserved() / 1024**3\n",
        "            total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "            return {\n",
        "                \"allocated_gb\": allocated,\n",
        "                \"cached_gb\": cached,\n",
        "                \"total_gb\": total,\n",
        "                \"free_gb\": total - allocated\n",
        "            }\n",
        "        return {\"error\": \"CUDA not available\"}\n",
        "\n",
        "    @staticmethod\n",
        "    def optimize_for_device():\n",
        "        if torch.cuda.is_available():\n",
        "            device_name = torch.cuda.get_device_name().lower()\n",
        "            if \"a100\" in device_name:\n",
        "                return {\n",
        "                    \"batch_size\": 1,\n",
        "                    \"gradient_accumulation_steps\": 8,\n",
        "                    \"max_length\": 512,\n",
        "                    \"lora_r\": 2,\n",
        "                    \"use_gradient_checkpointing\": True,\n",
        "                    \"fp16\": False,\n",
        "                    \"bf16\": True\n",
        "                }\n",
        "            else:\n",
        "                return {\n",
        "                    \"batch_size\": 1,\n",
        "                    \"gradient_accumulation_steps\": 16,\n",
        "                    \"max_length\": 256,\n",
        "                    \"lora_r\": 1,\n",
        "                    \"use_gradient_checkpointing\": True,\n",
        "                    \"fp16\": True,\n",
        "                    \"bf16\": False\n",
        "                }\n",
        "        return {\n",
        "            \"batch_size\": 1,\n",
        "            \"gradient_accumulation_steps\": 32,\n",
        "            \"max_length\": 256,\n",
        "            \"lora_r\": 1,\n",
        "            \"use_gradient_checkpointing\": True,\n",
        "            \"fp16\": False,\n",
        "            \"bf16\": False\n",
        "        }\n",
        "\n",
        "memory_manager = GPUMemoryManager()\n",
        "memory_manager.clear_memory()\n",
        "optimal_config = memory_manager.optimize_for_device()\n",
        "\n",
        "print(\"Optimal configuration:\")\n",
        "for key, value in optimal_config.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "# Configuration for Typhoon 2.1 Gemma3 4B\n",
        "config = {\n",
        "    \"model_name\": \"scb10x/typhoon2.1-gemma3-4b\",\n",
        "    \"output_dir\": \"/content/drive/MyDrive/V89Technology/typhoon21-gemma3-4b-medCare-finetuned\",\n",
        "    \"max_length\": optimal_config[\"max_length\"],\n",
        "    \"batch_size\": optimal_config[\"batch_size\"],\n",
        "    \"gradient_accumulation_steps\": optimal_config[\"gradient_accumulation_steps\"],\n",
        "    \"learning_rate\": 2e-4,\n",
        "    \"num_epochs\": 2,\n",
        "    \"warmup_ratio\": 0.05,\n",
        "    \"logging_steps\": 5,\n",
        "    \"save_steps\": 10,\n",
        "    \"eval_steps\": 10,\n",
        "    \"lora_r\": optimal_config[\"lora_r\"],\n",
        "    \"lora_alpha\": optimal_config[\"lora_r\"] * 2,\n",
        "    \"lora_dropout\": 0.1,\n",
        "    \"use_gradient_checkpointing\": optimal_config[\"use_gradient_checkpointing\"],\n",
        "    \"fp16\": optimal_config[\"fp16\"],\n",
        "    \"bf16\": optimal_config[\"bf16\"],\n",
        "    \"weight_decay\": 0.01,\n",
        "    \"lr_scheduler_type\": \"cosine\",\n",
        "    \"seed\": 42\n",
        "}\n",
        "\n",
        "print(f\"Configuration ready. Output: {config['output_dir']}\")\n",
        "\n",
        "# FIXED: Compatible quantization configuration\n",
        "def get_quantization_config():\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16 if config[\"bf16\"] else torch.float16,\n",
        "        llm_int8_threshold=6.0,\n",
        "    )\n",
        "    return bnb_config\n",
        "\n",
        "# FIXED: LoRA configuration for Gemma3\n",
        "def get_lora_config():\n",
        "    return LoraConfig(\n",
        "        task_type=TaskType.CAUSAL_LM,\n",
        "        r=config[\"lora_r\"],\n",
        "        lora_alpha=config[\"lora_alpha\"],\n",
        "        lora_dropout=config[\"lora_dropout\"],\n",
        "        target_modules=[\n",
        "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "            \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "        ],\n",
        "        bias=\"none\",\n",
        "        inference_mode=False,\n",
        "        init_lora_weights=True,\n",
        "    )\n",
        "\n",
        "quantization_config = get_quantization_config()\n",
        "lora_config = get_lora_config()\n",
        "\n",
        "# FIXED: Dataset loading with proper Thai encoding\n",
        "def load_external_csv_datasets():\n",
        "    datasets = {}\n",
        "    dataset_files = {\n",
        "        \"mental_health\": {\n",
        "            \"path\": f\"{drive_path}/mental_health_thai_150.csv\",\n",
        "            \"columns\": [\"th_Context\", \"th_Response\"]\n",
        "        },\n",
        "        \"healthcare\": {\n",
        "            \"path\": f\"{drive_path}/healthcare_thai_150.csv\",\n",
        "            \"columns\": [\"th_instruction\", \"th_input\", \"th_output\"]\n",
        "        },\n",
        "        \"pubmed\": {\n",
        "            \"path\": f\"{drive_path}/pubmed_thai_150.csv\",\n",
        "            \"columns\": [\"th_input\", \"th_output\", \"th_instruction\"]\n",
        "        },\n",
        "        \"medical_qa\": {\n",
        "            \"path\": f\"{drive_path}/medical_qa_thai_150.csv\",\n",
        "            \"columns\": [\"th_instruction\", \"th_input\", \"th_output\"]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    for dataset_name, dataset_config in dataset_files.items():\n",
        "        try:\n",
        "            print(f\"Loading {dataset_name}...\")\n",
        "            df = pd.read_csv(dataset_config[\"path\"], encoding='utf-8')\n",
        "            print(f\"  Loaded {len(df)} samples\")\n",
        "\n",
        "            samples = []\n",
        "            for _, row in df.iterrows():\n",
        "                try:\n",
        "                    if dataset_name == \"mental_health\":\n",
        "                        sample = {\n",
        "                            \"instruction\": \"ให้คำปรึกษาด้านสุขภาพจิต\",\n",
        "                            \"input\": str(row.get('th_Context', '')).strip(),\n",
        "                            \"output\": str(row.get('th_Response', '')).strip(),\n",
        "                            \"dataset_type\": dataset_name\n",
        "                        }\n",
        "                    else:\n",
        "                        sample = {\n",
        "                            \"instruction\": str(row.get('th_instruction', 'ให้คำแนะนำด้านการแพทย์')).strip(),\n",
        "                            \"input\": str(row.get('th_input', '')).strip(),\n",
        "                            \"output\": str(row.get('th_output', '')).strip(),\n",
        "                            \"dataset_type\": dataset_name\n",
        "                        }\n",
        "\n",
        "                    # Validate sample\n",
        "                    if (sample.get(\"instruction\") and sample.get(\"output\") and\n",
        "                        len(sample[\"instruction\"]) >= 5 and len(sample[\"output\"]) >= 10):\n",
        "                        samples.append(sample)\n",
        "\n",
        "                except Exception as e:\n",
        "                    continue\n",
        "\n",
        "            datasets[dataset_name] = samples\n",
        "            print(f\"  {dataset_name}: {len(samples)} valid samples\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  Error loading {dataset_name}: {str(e)}\")\n",
        "            datasets[dataset_name] = []\n",
        "\n",
        "    return datasets\n",
        "\n",
        "def load_enhanced_medical_datasets():\n",
        "    print(\"Loading datasets...\")\n",
        "    external_datasets = load_external_csv_datasets()\n",
        "\n",
        "    all_samples = []\n",
        "    for dataset_name, samples in external_datasets.items():\n",
        "        all_samples.extend(samples)\n",
        "\n",
        "    print(f\"Total samples: {len(all_samples)}\")\n",
        "\n",
        "    # Add fallback if insufficient data\n",
        "    if len(all_samples) < 50:\n",
        "        print(\"Adding fallback samples...\")\n",
        "        fallback_samples = [\n",
        "            {\n",
        "                \"instruction\": \"อธิบายอาการและการรักษาโรคทั่วไปในประเทศไทย\",\n",
        "                \"input\": \"โรคไข้เลือดออกมีอาการอย่างไร\",\n",
        "                \"output\": \"โรคไข้เลือดออกเกิดจากไวรัสเดงกี่ มีอาการไข้สูง ปวดหัว ปวดกล้ามเนื้อ คลื่นไส้อาเจียน ผื่นแดง อาจมีเลือดออกตามรูพรุ้น หากรุนแรงอาจเป็นไข้เลือดออกแบบช็อค ควรดื่มน้ำมากๆ พักผ่อน หลีกเลี่ยงยาแอสไพริน และรีบพบแพทย์ทันที\",\n",
        "                \"dataset_type\": \"fallback\"\n",
        "            },\n",
        "            {\n",
        "                \"instruction\": \"วิเคราะห์อาการและให้คำแนะนำเบื้องต้น\",\n",
        "                \"input\": \"มีอาการปวดท้อง ท้องเสีย เป็นมา 2 วัน\",\n",
        "                \"output\": \"อาการปวดท้องและท้องเสียอาจเกิดจากการติดเชื้อในทางเดินอาหาร การกินอาหารเป็นพิษ หรือความเครียด ควรดื่มน้ำสะอาดมากๆ กิน ORS หลีกเลี่ยงอาหารมัน เผ็ด หรือยา กินข้าวต้มหรืออาหารอ่อนๆ หากไม่ดีขึ้น ภายใน 1-2 วัน หรือมีไข้ ควรพบแพทย์\",\n",
        "                \"dataset_type\": \"fallback\"\n",
        "            },\n",
        "            {\n",
        "                \"instruction\": \"ให้คำปรึกษาด้านสุขภาพจิต\",\n",
        "                \"input\": \"รู้สึกเครียด นอนไม่หลับ กังวลเกี่ยวกับงาน\",\n",
        "                \"output\": \"ความเครียดจากงานเป็นเรื่องปกติ แต่ต้องจัดการอย่างเหมาะสม ควรหาเวลาพักผ่อน ออกกำลังกาย ทำสมาธิ หรือทำกิจกรรมที่ชอบ หลีกเลี่ยงคาเฟอีนก่อนนอน สร้างสภาพแวดล้อมที่เหมาะสำหรับการนอน หากอาการไม่ดีขึ้น ควรปรึกษาผู้เชี่ยวชาญด้านสุขภาพจิต\",\n",
        "                \"dataset_type\": \"fallback\"\n",
        "            }\n",
        "        ]\n",
        "        all_samples.extend(fallback_samples)\n",
        "\n",
        "    return {\"combined_dataset\": all_samples}\n",
        "\n",
        "# Load datasets\n",
        "print(\"Loading enhanced datasets...\")\n",
        "medical_datasets = load_enhanced_medical_datasets()\n",
        "\n",
        "all_samples = []\n",
        "for dataset_name, samples in medical_datasets.items():\n",
        "    valid_samples = [s for s in samples if isinstance(s, dict) and s.get(\"output\") and len(str(s[\"output\"]).strip()) >= 10]\n",
        "    all_samples.extend(valid_samples)\n",
        "\n",
        "print(f\"Total valid samples: {len(all_samples)}\")\n",
        "\n",
        "# Train/validation/test split\n",
        "def create_split(samples, train_ratio=0.8, val_ratio=0.1):\n",
        "    import random\n",
        "    random.seed(config[\"seed\"])\n",
        "    random.shuffle(samples)\n",
        "\n",
        "    total = len(samples)\n",
        "    train_end = int(total * train_ratio)\n",
        "    val_end = train_end + int(total * val_ratio)\n",
        "\n",
        "    return samples[:train_end], samples[train_end:val_end], samples[val_end:]\n",
        "\n",
        "train_samples, val_samples, test_samples = create_split(all_samples)\n",
        "\n",
        "print(f\"Training: {len(train_samples)}\")\n",
        "print(f\"Validation: {len(val_samples)}\")\n",
        "print(f\"Test: {len(test_samples)}\")\n",
        "\n",
        "if len(train_samples) < 10:\n",
        "    raise ValueError(\"Insufficient training samples\")\n",
        "\n",
        "# FIXED: Load tokenizer with proper configuration\n",
        "print(\"Loading model and tokenizer...\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    config[\"model_name\"],\n",
        "    trust_remote_code=True,\n",
        "    padding_side='right',\n",
        "    use_fast=True,\n",
        ")\n",
        "\n",
        "# FIXED: Configure pad token properly\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "print(\"Loading model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    config[\"model_name\"],\n",
        "    quantization_config=quantization_config,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16 if config[\"bf16\"] else torch.float16,\n",
        "    use_cache=False,\n",
        "    attn_implementation=\"flash_attention_2\" if torch.cuda.is_available() else \"eager\"\n",
        ")\n",
        "\n",
        "# Enable gradient checkpointing\n",
        "if config[\"use_gradient_checkpointing\"]:\n",
        "    model.gradient_checkpointing_enable()\n",
        "\n",
        "# FIXED: Prepare for training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "def print_trainable_parameters(model):\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(f\"Trainable: {trainable_params:,} || All: {all_param:,} || Ratio: {100 * trainable_params / all_param:.2f}%\")\n",
        "\n",
        "print_trainable_parameters(model)\n",
        "\n",
        "# FIXED: Data preprocessing with proper tokenization\n",
        "def format_training_sample(sample):\n",
        "    instruction = str(sample.get(\"instruction\", \"\")).strip()\n",
        "    input_text = str(sample.get(\"input\", \"\")).strip()\n",
        "    output_text = str(sample.get(\"output\", \"\")).strip()\n",
        "\n",
        "    if input_text:\n",
        "        prompt = f\"### คำสั่ง:\\n{instruction}\\n\\n### คำถาม:\\n{input_text}\\n\\n### คำตอบ:\\n{output_text}\"\n",
        "    else:\n",
        "        prompt = f\"### คำสั่ง:\\n{instruction}\\n\\n### คำตอบ:\\n{output_text}\"\n",
        "\n",
        "    return prompt\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    # FIXED: Handle batch processing correctly\n",
        "    formatted_texts = []\n",
        "    for i in range(len(examples[\"instruction\"])):\n",
        "        sample = {\n",
        "            \"instruction\": examples[\"instruction\"][i],\n",
        "            \"input\": examples.get(\"input\", [\"\"] * len(examples[\"instruction\"]))[i],\n",
        "            \"output\": examples[\"output\"][i]\n",
        "        }\n",
        "        formatted_texts.append(format_training_sample(sample))\n",
        "\n",
        "    # FIXED: Proper tokenization with attention masks\n",
        "    tokenized = tokenizer(\n",
        "        formatted_texts,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=config[\"max_length\"],\n",
        "        return_tensors=None,\n",
        "        add_special_tokens=True\n",
        "    )\n",
        "\n",
        "    # FIXED: Create proper labels and attention masks\n",
        "    tokenized[\"labels\"] = []\n",
        "    tokenized[\"attention_mask\"] = []\n",
        "\n",
        "    for i, input_ids in enumerate(tokenized[\"input_ids\"]):\n",
        "        # Create attention mask\n",
        "        attention_mask = [1 if token_id != tokenizer.pad_token_id else 0 for token_id in input_ids]\n",
        "        tokenized[\"attention_mask\"].append(attention_mask)\n",
        "\n",
        "        # Create labels (same as input_ids for causal LM)\n",
        "        labels = input_ids.copy()\n",
        "        tokenized[\"labels\"].append(labels)\n",
        "\n",
        "    return tokenized\n",
        "\n",
        "# FIXED: Convert to dataset format\n",
        "def samples_to_dataset_dict(samples):\n",
        "    return {\n",
        "        \"instruction\": [s.get(\"instruction\", \"\") for s in samples],\n",
        "        \"input\": [s.get(\"input\", \"\") for s in samples],\n",
        "        \"output\": [s.get(\"output\", \"\") for s in samples],\n",
        "        \"dataset_type\": [s.get(\"dataset_type\", \"general\") for s in samples]\n",
        "    }\n",
        "\n",
        "print(\"Creating datasets...\")\n",
        "train_dataset = Dataset.from_dict(samples_to_dataset_dict(train_samples))\n",
        "val_dataset = Dataset.from_dict(samples_to_dataset_dict(val_samples))\n",
        "test_dataset = Dataset.from_dict(samples_to_dataset_dict(test_samples))\n",
        "\n",
        "print(\"Preprocessing...\")\n",
        "train_dataset = train_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=train_dataset.column_names,\n",
        "    desc=\"Processing training data\"\n",
        ")\n",
        "\n",
        "val_dataset = val_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=val_dataset.column_names,\n",
        "    desc=\"Processing validation data\"\n",
        ")\n",
        "\n",
        "test_dataset = test_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=test_dataset.column_names,\n",
        "    desc=\"Processing test data\"\n",
        ")\n",
        "\n",
        "print(f\"Processed - Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
        "\n",
        "# FIXED: Data collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,\n",
        "    pad_to_multiple_of=8,\n",
        ")\n",
        "\n",
        "# FIXED: Compute metrics function\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "\n",
        "    if isinstance(predictions, tuple):\n",
        "        predictions = predictions[0]\n",
        "\n",
        "    # Convert to numpy if needed\n",
        "    if hasattr(predictions, 'cpu'):\n",
        "        predictions = predictions.cpu().numpy()\n",
        "    if hasattr(labels, 'cpu'):\n",
        "        labels = labels.cpu().numpy()\n",
        "\n",
        "    # Flatten labels and get mask for non-ignored tokens\n",
        "    labels_flat = labels.flatten()\n",
        "    mask = labels_flat != -100\n",
        "\n",
        "    if mask.sum() == 0:\n",
        "        return {\"perplexity\": float('inf'), \"eval_loss\": float('inf')}\n",
        "\n",
        "    # Calculate perplexity from logits\n",
        "    valid_predictions = predictions.reshape(-1, predictions.shape[-1])\n",
        "    valid_labels = labels_flat[mask]\n",
        "\n",
        "    # Calculate cross-entropy loss\n",
        "    log_probs = F.log_softmax(torch.tensor(valid_predictions[mask]), dim=-1)\n",
        "    nll_loss = F.nll_loss(log_probs, torch.tensor(valid_labels), reduction='mean')\n",
        "    perplexity = torch.exp(nll_loss).item()\n",
        "\n",
        "    return {\n",
        "        \"perplexity\": min(perplexity, 10000.0),  # Cap perplexity\n",
        "        \"eval_loss\": nll_loss.item()\n",
        "    }\n",
        "\n",
        "# FIXED: Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=config[\"output_dir\"],\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=config[\"num_epochs\"],\n",
        "    per_device_train_batch_size=config[\"batch_size\"],\n",
        "    per_device_eval_batch_size=config[\"batch_size\"],\n",
        "    gradient_accumulation_steps=config[\"gradient_accumulation_steps\"],\n",
        "    gradient_checkpointing=config[\"use_gradient_checkpointing\"],\n",
        "    learning_rate=config[\"learning_rate\"],\n",
        "    weight_decay=config[\"weight_decay\"],\n",
        "    lr_scheduler_type=config[\"lr_scheduler_type\"],\n",
        "    warmup_ratio=config[\"warmup_ratio\"],\n",
        "    logging_steps=config[\"logging_steps\"],\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=config[\"eval_steps\"],\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=config[\"save_steps\"],\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    fp16=config[\"fp16\"],\n",
        "    bf16=config[\"bf16\"],\n",
        "    dataloader_pin_memory=True,\n",
        "    dataloader_num_workers=2,\n",
        "    remove_unused_columns=False,\n",
        "    report_to=None,\n",
        "    run_name=\"typhoon21_gemma3_4b_medical_fixed_v5\",\n",
        "    seed=config[\"seed\"],\n",
        "    data_seed=config[\"seed\"],\n",
        "    group_by_length=False,\n",
        "    dataloader_drop_last=False,\n",
        "    eval_accumulation_steps=1,\n",
        "    prediction_loss_only=False,\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "# Training callback\n",
        "class EnhancedCallback(TrainerCallback):\n",
        "    def __init__(self):\n",
        "        self.training_started = False\n",
        "\n",
        "    def on_train_begin(self, args, state, control, **kwargs):\n",
        "        self.training_started = True\n",
        "        print(\"Training Started!\")\n",
        "        print(f\"Configuration: {args.num_train_epochs} epochs, batch size {args.per_device_train_batch_size}\")\n",
        "\n",
        "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
        "        current_epoch = int(state.epoch) + 1 if state.epoch is not None else 1\n",
        "        print(f\"\\nEpoch {current_epoch}/{args.num_train_epochs}\")\n",
        "        memory_manager.clear_memory()\n",
        "\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        if logs and self.training_started:\n",
        "            step = logs.get('step', state.global_step)\n",
        "            if 'loss' in logs:\n",
        "                print(f\"Step {step}: Loss = {logs['loss']:.4f}\")\n",
        "            if 'eval_loss' in logs:\n",
        "                print(f\"Step {step}: Eval Loss = {logs['eval_loss']:.4f}\")\n",
        "\n",
        "    def on_train_end(self, args, state, control, **kwargs):\n",
        "        print(\"Training Completed!\")\n",
        "\n",
        "# Initialize trainer\n",
        "print(\"Initializing trainer...\")\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[\n",
        "        EnhancedCallback(),\n",
        "        EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.001)\n",
        "    ],\n",
        ")\n",
        "\n",
        "# FIXED: Generation function\n",
        "def generate_response(model, tokenizer, instruction, input_text=\"\", max_length=256, temperature=0.7, top_p=0.9):\n",
        "    \"\"\"Fixed generation with proper input handling\"\"\"\n",
        "    if input_text:\n",
        "        prompt = f\"### คำสั่ง:\\n{instruction}\\n\\n### คำถาม:\\n{input_text}\\n\\n### คำตอบ:\\n\"\n",
        "    else:\n",
        "        prompt = f\"### คำสั่ง:\\n{instruction}\\n\\n### คำตอบ:\\n\"\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        padding=False\n",
        "    )\n",
        "\n",
        "    # FIXED: Ensure inputs are on correct device and dtype\n",
        "    if torch.cuda.is_available():\n",
        "        device = next(model.parameters()).device\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        # FIXED: Convert input_ids to long tensor to avoid dtype error\n",
        "        inputs['input_ids'] = inputs['input_ids'].long()\n",
        "        if 'attention_mask' in inputs:\n",
        "            inputs['attention_mask'] = inputs['attention_mask'].long()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        generation_config = GenerationConfig(\n",
        "            max_new_tokens=128,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            repetition_penalty=1.1,\n",
        "            no_repeat_ngram_size=3,\n",
        "        )\n",
        "\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            generation_config=generation_config\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    if \"### คำตอบ:\\n\" in response:\n",
        "        response = response.split(\"### คำตอบ:\\n\")[1].strip()\n",
        "\n",
        "    return response\n",
        "\n",
        "def evaluate_model_performance(model, tokenizer, test_samples, num_samples=3):\n",
        "    \"\"\"Evaluate model performance\"\"\"\n",
        "    print(f\"Evaluating on {min(num_samples, len(test_samples))} samples...\")\n",
        "\n",
        "    results = []\n",
        "    for i, sample in enumerate(test_samples[:num_samples]):\n",
        "        instruction = sample.get(\"instruction\", \"\")\n",
        "        input_text = sample.get(\"input\", \"\")\n",
        "        expected = sample.get(\"output\", \"\")\n",
        "\n",
        "        print(f\"\\n--- Sample {i+1} ---\")\n",
        "        print(f\"Instruction: {instruction}\")\n",
        "        if input_text:\n",
        "            print(f\"Input: {input_text}\")\n",
        "        print(f\"Expected: {expected[:100]}...\")\n",
        "\n",
        "        try:\n",
        "            generated = generate_response(\n",
        "                model, tokenizer, instruction, input_text,\n",
        "                max_length=config[\"max_length\"], temperature=0.7\n",
        "            )\n",
        "            print(f\"Generated: {generated[:100]}...\")\n",
        "\n",
        "            results.append({\n",
        "                \"instruction\": instruction,\n",
        "                \"input\": input_text,\n",
        "                \"expected\": expected,\n",
        "                \"generated\": generated\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"Generation error: {e}\")\n",
        "            results.append({\n",
        "                \"instruction\": instruction,\n",
        "                \"input\": input_text,\n",
        "                \"expected\": expected,\n",
        "                \"generated\": f\"Error: {str(e)}\"\n",
        "            })\n",
        "\n",
        "    return results\n",
        "\n",
        "# Pre-training evaluation\n",
        "print(\"\\n=== Pre-training Evaluation ===\")\n",
        "pre_results = evaluate_model_performance(model, tokenizer, test_samples, 2)\n",
        "\n",
        "# Clear memory and start training\n",
        "memory_manager.clear_memory()\n",
        "\n",
        "print(\"\\n=== Starting Training ===\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "try:\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "    print(\"Training completed successfully!\")\n",
        "\n",
        "    # Save the final model\n",
        "    print(\"\\n=== Saving Model ===\")\n",
        "    trainer.save_model()\n",
        "    tokenizer.save_pretrained(config[\"output_dir\"])\n",
        "\n",
        "    # Save training state\n",
        "    trainer.save_state()\n",
        "\n",
        "    print(f\"Model saved to: {config['output_dir']}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Training failed: {str(e)}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "    # Save partial progress if possible\n",
        "    try:\n",
        "        print(\"Attempting to save partial progress...\")\n",
        "        trainer.save_model(output_dir=f\"{config['output_dir']}_partial\")\n",
        "        print(\"Partial model saved successfully\")\n",
        "    except Exception as save_error:\n",
        "        print(f\"Failed to save partial model: {str(save_error)}\")\n",
        "\n",
        "# Clear memory after training\n",
        "memory_manager.clear_memory()\n",
        "\n",
        "# Post-training evaluation\n",
        "print(\"\\n=== Post-training Evaluation ===\")\n",
        "try:\n",
        "    # Reload the trained model for evaluation\n",
        "    print(\"Loading trained model for evaluation...\")\n",
        "\n",
        "    # Load the fine-tuned model\n",
        "    trained_model = PeftModel.from_pretrained(\n",
        "        model,\n",
        "        config[\"output_dir\"],\n",
        "        torch_dtype=torch.bfloat16 if config[\"bf16\"] else torch.float16,\n",
        "    )\n",
        "    trained_model.eval()\n",
        "\n",
        "    print(\"Evaluating trained model performance...\")\n",
        "    post_results = evaluate_model_performance(trained_model, tokenizer, test_samples, 5)\n",
        "\n",
        "    # Compare results\n",
        "    print(\"\\n=== Training Results Comparison ===\")\n",
        "    print(\"Pre-training vs Post-training samples:\")\n",
        "\n",
        "    for i, (pre, post) in enumerate(zip(pre_results[:2], post_results[:2])):\n",
        "        print(f\"\\n--- Comparison {i+1} ---\")\n",
        "        print(f\"Instruction: {pre['instruction']}\")\n",
        "        if pre['input']:\n",
        "            print(f\"Input: {pre['input']}\")\n",
        "        print(f\"Expected: {pre['expected'][:100]}...\")\n",
        "        print(f\"Pre-training:  {pre['generated'][:100]}...\")\n",
        "        print(f\"Post-training: {post['generated'][:100]}...\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Post-training evaluation failed: {str(e)}\")\n",
        "\n",
        "# Model testing function\n",
        "def test_medical_model(model, tokenizer):\n",
        "    \"\"\"Test the model with medical scenarios\"\"\"\n",
        "    print(\"\\n=== Medical Model Testing ===\")\n",
        "\n",
        "    test_cases = [\n",
        "        {\n",
        "            \"instruction\": \"ให้คำแนะนำด้านการแพทย์\",\n",
        "            \"input\": \"มีอาการไข้ ปวดหัว เจ็บคอ มา 2 วัน\",\n",
        "            \"description\": \"Common cold symptoms\"\n",
        "        },\n",
        "        {\n",
        "            \"instruction\": \"ให้คำปรึกษาด้านสุขภาพจิต\",\n",
        "            \"input\": \"รู้สึกเครียดและกังวลมาก ทำงานไม่ได้\",\n",
        "            \"description\": \"Mental health consultation\"\n",
        "        },\n",
        "        {\n",
        "            \"instruction\": \"อธิบายเกี่ยวกับโรคและการรักษา\",\n",
        "            \"input\": \"โรคเบาหวานคืออะไร\",\n",
        "            \"description\": \"Diabetes explanation\"\n",
        "        },\n",
        "        {\n",
        "            \"instruction\": \"ให้คำแนะนำการดูแลสุขภาพ\",\n",
        "            \"input\": \"วิธีดูแลสุขภาพหัวใจให้แข็งแรง\",\n",
        "            \"description\": \"Heart health advice\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    for i, test_case in enumerate(test_cases):\n",
        "        print(f\"\\n--- Test Case {i+1}: {test_case['description']} ---\")\n",
        "        print(f\"Instruction: {test_case['instruction']}\")\n",
        "        print(f\"Input: {test_case['input']}\")\n",
        "\n",
        "        try:\n",
        "            response = generate_response(\n",
        "                model, tokenizer,\n",
        "                test_case['instruction'],\n",
        "                test_case['input'],\n",
        "                max_length=512,\n",
        "                temperature=0.7\n",
        "            )\n",
        "            print(f\"Response: {response}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating response: {str(e)}\")\n",
        "\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "# Test the trained model\n",
        "try:\n",
        "    if 'trained_model' in locals():\n",
        "        test_medical_model(trained_model, tokenizer)\n",
        "    else:\n",
        "        print(\"Trained model not available for testing\")\n",
        "except Exception as e:\n",
        "    print(f\"Model testing failed: {str(e)}\")\n",
        "\n",
        "# Save training metrics and results\n",
        "def save_training_results():\n",
        "    \"\"\"Save training results and metrics\"\"\"\n",
        "    print(\"\\n=== Saving Training Results ===\")\n",
        "\n",
        "    try:\n",
        "        results_data = {\n",
        "            \"config\": config,\n",
        "            \"training_completed\": True,\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"model_name\": config[\"model_name\"],\n",
        "            \"output_dir\": config[\"output_dir\"],\n",
        "            \"dataset_info\": {\n",
        "                \"train_samples\": len(train_samples),\n",
        "                \"val_samples\": len(val_samples),\n",
        "                \"test_samples\": len(test_samples),\n",
        "                \"total_samples\": len(all_samples)\n",
        "            },\n",
        "            \"memory_info\": memory_manager.get_memory_info(),\n",
        "            \"optimal_config\": optimal_config\n",
        "        }\n",
        "\n",
        "        # Add training history if available\n",
        "        if hasattr(trainer, 'state') and trainer.state.log_history:\n",
        "            results_data[\"training_history\"] = trainer.state.log_history\n",
        "\n",
        "        # Add evaluation results\n",
        "        if 'pre_results' in locals():\n",
        "            results_data[\"pre_training_samples\"] = pre_results\n",
        "        if 'post_results' in locals():\n",
        "            results_data[\"post_training_samples\"] = post_results\n",
        "\n",
        "        # Save to JSON file\n",
        "        results_file = f\"{config['output_dir']}/training_results.json\"\n",
        "        os.makedirs(os.path.dirname(results_file), exist_ok=True)\n",
        "\n",
        "        with open(results_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(results_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"Training results saved to: {results_file}\")\n",
        "\n",
        "        # Create summary report\n",
        "        summary_file = f\"{config['output_dir']}/training_summary.txt\"\n",
        "        with open(summary_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(\"=== Typhoon 2.1 Gemma3 4B Medical Fine-tuning Summary ===\\n\")\n",
        "            f.write(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "            f.write(f\"Model: {config['model_name']}\\n\")\n",
        "            f.write(f\"Output Directory: {config['output_dir']}\\n\")\n",
        "            f.write(f\"Training Samples: {len(train_samples)}\\n\")\n",
        "            f.write(f\"Validation Samples: {len(val_samples)}\\n\")\n",
        "            f.write(f\"Test Samples: {len(test_samples)}\\n\")\n",
        "            f.write(f\"Epochs: {config['num_epochs']}\\n\")\n",
        "            f.write(f\"Learning Rate: {config['learning_rate']}\\n\")\n",
        "            f.write(f\"LoRA Rank: {config['lora_r']}\\n\")\n",
        "            f.write(f\"Max Length: {config['max_length']}\\n\")\n",
        "            f.write(f\"Batch Size: {config['batch_size']}\\n\")\n",
        "            f.write(f\"Gradient Accumulation Steps: {config['gradient_accumulation_steps']}\\n\")\n",
        "\n",
        "            memory_info = memory_manager.get_memory_info()\n",
        "            if \"error\" not in memory_info:\n",
        "                f.write(f\"GPU Memory Used: {memory_info['allocated_gb']:.2f} GB\\n\")\n",
        "                f.write(f\"GPU Memory Total: {memory_info['total_gb']:.2f} GB\\n\")\n",
        "\n",
        "            f.write(\"\\n=== Configuration ===\\n\")\n",
        "            for key, value in config.items():\n",
        "                f.write(f\"{key}: {value}\\n\")\n",
        "\n",
        "        print(f\"Training summary saved to: {summary_file}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to save training results: {str(e)}\")\n",
        "\n",
        "save_training_results()\n",
        "\n",
        "# Create inference function for deployment\n",
        "def create_inference_function():\n",
        "    \"\"\"Create a standalone inference function\"\"\"\n",
        "    print(\"\\n=== Creating Inference Function ===\")\n",
        "\n",
        "    inference_code = '''\n",
        "def load_trained_model(model_path):\n",
        "    \"\"\"Load the trained model for inference\"\"\"\n",
        "    import torch\n",
        "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "    from peft import PeftModel\n",
        "\n",
        "    # Load base model and tokenizer\n",
        "    base_model_name = \"scb10x/typhoon2.1-gemma3-4b\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_name,\n",
        "        trust_remote_code=True,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    # Load fine-tuned weights\n",
        "    model = PeftModel.from_pretrained(base_model, model_path)\n",
        "    model.eval()\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "def medical_chat_inference(model, tokenizer, instruction, input_text=\"\", max_length=256):\n",
        "    \"\"\"Generate medical advice response\"\"\"\n",
        "    if input_text:\n",
        "        prompt = f\"### คำสั่ง:\\\\n{instruction}\\\\n\\\\n### คำถาม:\\\\n{input_text}\\\\n\\\\n### คำตอบ:\\\\n\"\n",
        "    else:\n",
        "        prompt = f\"### คำสั่ง:\\\\n{instruction}\\\\n\\\\n### คำตอบ:\\\\n\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        device = next(model.parameters()).device\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        inputs['input_ids'] = inputs['input_ids'].long()\n",
        "        if 'attention_mask' in inputs:\n",
        "            inputs['attention_mask'] = inputs['attention_mask'].long()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=128,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            repetition_penalty=1.1,\n",
        "            no_repeat_ngram_size=3\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    if \"### คำตอบ:\\\\n\" in response:\n",
        "        response = response.split(\"### คำตอบ:\\\\n\")[1].strip()\n",
        "\n",
        "    return response\n",
        "\n",
        "# Example usage:\n",
        "# model, tokenizer = load_trained_model(\"path/to/your/model\")\n",
        "# response = medical_chat_inference(model, tokenizer, \"ให้คำแนะนำด้านการแพทย์\", \"มีอาการไข้\")\n",
        "    '''\n",
        "\n",
        "    try:\n",
        "        inference_file = f\"{config['output_dir']}/inference.py\"\n",
        "        with open(inference_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(inference_code)\n",
        "        print(f\"Inference function saved to: {inference_file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to save inference function: {str(e)}\")\n",
        "\n",
        "create_inference_function()\n",
        "\n",
        "# Final cleanup and summary\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"=== FINE-TUNING COMPLETE ===\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"✓ Model: {config['model_name']}\")\n",
        "print(f\"✓ Training samples: {len(train_samples)}\")\n",
        "print(f\"✓ Validation samples: {len(val_samples)}\")\n",
        "print(f\"✓ Test samples: {len(test_samples)}\")\n",
        "print(f\"✓ Epochs: {config['num_epochs']}\")\n",
        "print(f\"✓ Output directory: {config['output_dir']}\")\n",
        "\n",
        "memory_info = memory_manager.get_memory_info()\n",
        "if \"error\" not in memory_info:\n",
        "    print(f\"✓ Final GPU memory usage: {memory_info['allocated_gb']:.2f} GB / {memory_info['total_gb']:.2f} GB\")\n",
        "\n",
        "print(f\"✓ Model files saved successfully\")\n",
        "print(f\"✓ Training results and metrics saved\")\n",
        "print(f\"✓ Inference function created\")\n",
        "\n",
        "print(\"\\n=== Files Created ===\")\n",
        "print(f\"• Model files: {config['output_dir']}/\")\n",
        "print(f\"• Training results: {config['output_dir']}/training_results.json\")\n",
        "print(f\"• Training summary: {config['output_dir']}/training_summary.txt\")\n",
        "print(f\"• Inference function: {config['output_dir']}/inference.py\")\n",
        "\n",
        "print(\"\\n=== Next Steps ===\")\n",
        "print(\"1. Test the model with medical queries\")\n",
        "print(\"2. Deploy for inference using the provided inference.py\")\n",
        "print(\"3. Monitor performance and gather feedback\")\n",
        "print(\"4. Consider additional fine-tuning if needed\")\n",
        "\n",
        "# Final memory cleanup\n",
        "memory_manager.clear_memory()\n",
        "\n",
        "print(\"\\n🎉 Enhanced Typhoon 2.1 Gemma3 4B Medical Fine-tuning Completed Successfully! 🎉\")\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "UgJAk6ihbdW5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}