{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpvxeeUgoMzG"
      },
      "outputs": [],
      "source": [
        "# Enhanced Fine-tuning OpenThaiGPT 1.5-7B for Thai Medical Applications\n",
        "# Company: V89 Technology Ltd.\n",
        "# Version: 4.0 - FIXED External Dataset Loading & Processing Issues\n",
        "\n",
        "print(\"Starting Enhanced OpenThaiGPT Medical Fine-tuning Setup - FIXED EXTERNAL DATASET VERSION...\")\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import os\n",
        "import re\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
        "\n",
        "# Compatible package versions for A100 with Python 3.12.11 transformers>=4.44.2, CUDA 12.4\n",
        "!pip install -q --upgrade pip==24.0\n",
        "!pip install -q --force-reinstall torch==2.5.0 torchvision==0.20.0 torchaudio==2.5.0 --index-url https://download.pytorch.org/whl/cu124\n",
        "!pip install -q transformers==4.44.2 accelerate==0.33.0 bitsandbytes==0.43.3\n",
        "!pip install -q datasets==2.20.0 evaluate==0.4.2 rouge-score==0.1.2\n",
        "!pip install -q peft==0.12.0\n",
        "!pip install -q scikit-learn pandas numpy==2.0.2 scipy>=1.14.1 fsspec filelock typing-extensions nltk deepspeed\n",
        "!pip install -q sacrebleu\n",
        "\n",
        "# Import required libraries with fixed versions\n",
        "import sys\n",
        "import json\n",
        "import gc\n",
        "import torch\n",
        "import transformers\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from datasets import Dataset, load_dataset, DatasetDict\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from typing import Dict, List, Any, Optional, Union\n",
        "import torch.nn.functional as F\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "import math\n",
        "import torch.optim as optim\n",
        "from sacrebleu import corpus_bleu\n",
        "from collections import defaultdict\n",
        "\n",
        "# FIXED: Updated imports for compatibility\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    EarlyStoppingCallback,\n",
        "    GenerationConfig,\n",
        "    get_linear_schedule_with_warmup,\n",
        "    TrainerCallback,\n",
        "    TrainerControl,\n",
        "    TrainerState,\n",
        "    EvalPrediction\n",
        ")\n",
        "from peft import (\n",
        "    get_peft_model,\n",
        "    LoraConfig,\n",
        "    TaskType,\n",
        "    PeftModel,\n",
        "    prepare_model_for_kbit_training,\n",
        "    get_peft_config,\n",
        "    PeftType\n",
        ")\n",
        "\n",
        "print(f\"===== Version Check =====\")\n",
        "print(f\"Python version: {sys.version}\")\n",
        "print(\"Transformers version:\", transformers.__version__)\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Numpy version: {np.__version__}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA Device: {torch.cuda.get_device_name()}\")\n",
        "    print(f\"CUDA Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "print(f\"==================\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive with error handling\n",
        "def setup_drive_connection():\n",
        "    \"\"\"Setup Google Drive connection with fallback\"\"\"\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        drive_path = \"/content/drive/MyDrive/V89Technology/thai_medicalCare_dataset150\"\n",
        "        print(\"Google Drive connected successfully\")\n",
        "        return drive_path\n",
        "    except:\n",
        "        local_path = \"V89Technology/thai_medicalCare_dataset150\"\n",
        "        os.makedirs(local_path, exist_ok=True)\n",
        "        print(\"Running outside Colab - using local directory\")\n",
        "        return local_path\n",
        "\n",
        "drive_path = setup_drive_connection()\n",
        "\n",
        "# FIXED: Enhanced GPU memory management with A100 optimizations\n",
        "class GPUMemoryManager:\n",
        "    \"\"\"Advanced GPU memory management class with A100 optimizations\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def clear_memory():\n",
        "        \"\"\"Comprehensive GPU memory clearing\"\"\"\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            torch.cuda.synchronize()\n",
        "            torch.cuda.reset_peak_memory_stats()\n",
        "            torch.cuda.reset_accumulated_memory_stats()\n",
        "\n",
        "    @staticmethod\n",
        "    def get_memory_info():\n",
        "        \"\"\"Get current GPU memory usage\"\"\"\n",
        "        if torch.cuda.is_available():\n",
        "            allocated = torch.cuda.memory_allocated() / 1024**3\n",
        "            cached = torch.cuda.memory_reserved() / 1024**3\n",
        "            total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "\n",
        "            return {\n",
        "                \"allocated_gb\": allocated,\n",
        "                \"cached_gb\": cached,\n",
        "                \"total_gb\": total,\n",
        "                \"free_gb\": total - allocated\n",
        "            }\n",
        "        return {\"error\": \"CUDA not available\"}\n",
        "\n",
        "    @staticmethod\n",
        "    def optimize_for_device():\n",
        "        \"\"\"FIXED: Optimized settings for A100 40GB based on training report\"\"\"\n",
        "        if torch.cuda.is_available():\n",
        "            device_name = torch.cuda.get_device_name().lower()\n",
        "            memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "\n",
        "            if \"a100\" in device_name:\n",
        "                return {\n",
        "                    \"batch_size\": 16,\n",
        "                    \"gradient_accumulation_steps\": 4,\n",
        "                    \"max_length\": 512,\n",
        "                    \"lora_r\": 4,\n",
        "                    \"use_gradient_checkpointing\": False,\n",
        "                    \"fp16\": False,\n",
        "                    \"bf16\": True\n",
        "                }\n",
        "            else:  # T4 or other GPUs\n",
        "                return {\n",
        "                    \"batch_size\": 1,\n",
        "                    \"gradient_accumulation_steps\": 12,\n",
        "                    \"max_length\": 256,\n",
        "                    \"lora_r\": 1,\n",
        "                    \"use_gradient_checkpointing\": True,\n",
        "                    \"fp16\": True,\n",
        "                    \"bf16\": False\n",
        "                }\n",
        "        return {\n",
        "            \"batch_size\": 1,\n",
        "            \"gradient_accumulation_steps\": 12,\n",
        "            \"max_length\": 256,\n",
        "            \"lora_r\": 1,\n",
        "            \"use_gradient_checkpointing\": True,\n",
        "            \"fp16\": False,\n",
        "            \"bf16\": False\n",
        "        }\n",
        "\n",
        "# Initialize memory manager and get optimal config\n",
        "memory_manager = GPUMemoryManager()\n",
        "memory_manager.clear_memory()\n",
        "optimal_config = memory_manager.optimize_for_device()\n",
        "\n",
        "print(\"Optimal configuration based on hardware:\")\n",
        "for key, value in optimal_config.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "# FIXED: Configuration based on training report analysis\n",
        "config = {\n",
        "    \"model_name\": \"openthaigpt/openthaigpt1.5-7b-instruct\",\n",
        "    \"output_dir\": \"/content/drive/MyDrive/V89Technology/openthaigpt15-7b-medCare-finetuned\",\n",
        "    \"max_length\": optimal_config[\"max_length\"],\n",
        "    \"batch_size\": optimal_config[\"batch_size\"],\n",
        "    \"gradient_accumulation_steps\": optimal_config[\"gradient_accumulation_steps\"],\n",
        "    \"learning_rate\": 0.0003,\n",
        "    \"num_epochs\": 3,\n",
        "    \"warmup_ratio\": 0.05,\n",
        "    \"logging_steps\": 5,\n",
        "    \"save_steps\": 5,\n",
        "    \"eval_steps\": 5,\n",
        "    \"lora_r\": optimal_config[\"lora_r\"],\n",
        "    \"lora_alpha\": 8,\n",
        "    \"lora_dropout\": 0.1,\n",
        "    \"use_gradient_checkpointing\": optimal_config[\"use_gradient_checkpointing\"],\n",
        "    \"fp16\": optimal_config[\"fp16\"],\n",
        "    \"bf16\": optimal_config[\"bf16\"],\n",
        "    \"weight_decay\": 0.01,\n",
        "    \"lr_scheduler_type\": \"cosine\",\n",
        "    \"seed\": 42\n",
        "}\n",
        "\n",
        "print(f\"Enhanced configuration ready. Output directory: {config['output_dir']}\")\n",
        "\n",
        "# FIXED: Advanced quantization configuration\n",
        "def get_advanced_quantization_config():\n",
        "    \"\"\"Get optimized quantization configuration for A100\"\"\"\n",
        "    return BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        llm_int8_threshold=6.0,\n",
        "    )\n",
        "\n",
        "# FIXED: Advanced LoRA configuration matching training report\n",
        "def get_advanced_lora_config():\n",
        "    \"\"\"Get optimized LoRA configuration based on training report\"\"\"\n",
        "    return LoraConfig(\n",
        "        task_type=TaskType.CAUSAL_LM,\n",
        "        r=config[\"lora_r\"],\n",
        "        lora_alpha=config[\"lora_alpha\"],\n",
        "        lora_dropout=config[\"lora_dropout\"],\n",
        "        target_modules=[\n",
        "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "            \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "        ],\n",
        "        bias=\"none\",\n",
        "        inference_mode=False,\n",
        "        init_lora_weights=True,\n",
        "        use_rslora=False,\n",
        "    )\n",
        "\n",
        "quantization_config = get_advanced_quantization_config()\n",
        "lora_config = get_advanced_lora_config()\n",
        "\n",
        "print(\"Advanced quantization and LoRA configuration ready\")\n",
        "\n",
        "# FIXED: Load external CSV datasets from Google Drive\n",
        "def load_external_csv_datasets():\n",
        "    \"\"\"Load and process external CSV datasets with Thai text columns\"\"\"\n",
        "    datasets = {}\n",
        "\n",
        "    # Define dataset file paths and configurations\n",
        "    dataset_files = {\n",
        "        \"medmcqa\": {\n",
        "            \"path\": f\"{drive_path}/medmcqa_thai_132.csv\",\n",
        "            \"thai_columns\": [\"th_question\", \"th_opa\", \"th_opb\", \"th_opc\", \"th_opd\", \"th_exp\"],\n",
        "            \"instruction_template\": \"‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏ó‡∏¢‡πå‡πÅ‡∏•‡∏∞‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•\",\n",
        "            \"input_template\": \"‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {question}\\n‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å:\\nA) {opa}\\nB) {opb}\\nC) {opc}\\nD) {opd}\",\n",
        "            \"output_template\": \"‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á: {cop_letter}\\n‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: {exp}\"\n",
        "        },\n",
        "        \"mental_health\": {\n",
        "            \"path\": f\"{drive_path}/mental_health_thai_150.csv\",\n",
        "            \"thai_columns\": [\"th_Context\", \"th_Response\"],\n",
        "            \"instruction_template\": \"‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡∏î‡πâ‡∏≤‡∏ô‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û‡∏à‡∏¥‡∏ï\",\n",
        "            \"input_template\": \"{context}\",\n",
        "            \"output_template\": \"{response}\"\n",
        "        },\n",
        "        \"healthcare\": {\n",
        "            \"path\": f\"{drive_path}/healthcare_thai_150.csv\",\n",
        "            \"thai_columns\": [\"th_instruction\", \"th_input\", \"th_output\"],\n",
        "            \"instruction_template\": \"{instruction}\",\n",
        "            \"input_template\": \"{input}\",\n",
        "            \"output_template\": \"{output}\"\n",
        "        },\n",
        "        \"pubmed\": {\n",
        "            \"path\": f\"{drive_path}/pubmed_thai_150.csv\",\n",
        "            \"thai_columns\": [\"th_input\", \"th_output\", \"th_instruction\"],\n",
        "            \"instruction_template\": \"{instruction}\",\n",
        "            \"input_template\": \"{input}\",\n",
        "            \"output_template\": \"{output}\"\n",
        "        },\n",
        "        \"medical_qa\": {\n",
        "            \"path\": f\"{drive_path}/medical_qa_thai_150.csv\",\n",
        "            \"thai_columns\": [\"th_instruction\", \"th_input\", \"th_output\"],\n",
        "            \"instruction_template\": \"{instruction}\",\n",
        "            \"input_template\": \"{input}\",\n",
        "            \"output_template\": \"{output}\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "    for dataset_name, dataset_config in dataset_files.items():\n",
        "        try:\n",
        "            print(f\"Loading {dataset_name} dataset...\")\n",
        "\n",
        "            # Read CSV file\n",
        "            df = pd.read_csv(dataset_config[\"path\"])\n",
        "            print(f\"  Loaded {len(df)} samples from {dataset_config['path']}\")\n",
        "\n",
        "            samples = []\n",
        "\n",
        "            for _, row in df.iterrows():\n",
        "                try:\n",
        "                    # Use Thai columns for processing\n",
        "                    sample_data = {}\n",
        "\n",
        "                    if dataset_name == \"medmcqa\":\n",
        "                        # Map choice number to letter\n",
        "                        cop_mapping = {1: \"A\", 2: \"B\", 3: \"C\", 4: \"D\"}\n",
        "                        cop_letter = cop_mapping.get(row.get('cop', 1), \"A\")\n",
        "\n",
        "                        sample_data = {\n",
        "                            \"instruction\": dataset_config[\"instruction_template\"],\n",
        "                            \"input\": dataset_config[\"input_template\"].format(\n",
        "                                question=row.get('th_question', ''),\n",
        "                                opa=row.get('th_opa', ''),\n",
        "                                opb=row.get('th_opb', ''),\n",
        "                                opc=row.get('th_opc', ''),\n",
        "                                opd=row.get('th_opd', '')\n",
        "                            ),\n",
        "                            \"output\": dataset_config[\"output_template\"].format(\n",
        "                                cop_letter=cop_letter,\n",
        "                                exp=row.get('th_exp', '')\n",
        "                            ),\n",
        "                            \"dataset_type\": dataset_name\n",
        "                        }\n",
        "\n",
        "                    elif dataset_name == \"mental_health\":\n",
        "                        sample_data = {\n",
        "                            \"instruction\": dataset_config[\"instruction_template\"],\n",
        "                            \"input\": dataset_config[\"input_template\"].format(\n",
        "                                context=row.get('th_Context', '')\n",
        "                            ),\n",
        "                            \"output\": dataset_config[\"output_template\"].format(\n",
        "                                response=row.get('th_Response', '')\n",
        "                            ),\n",
        "                            \"dataset_type\": dataset_name\n",
        "                        }\n",
        "\n",
        "                    else:  # healthcare, pubmed, medical_qa\n",
        "                        sample_data = {\n",
        "                            \"instruction\": row.get('th_instruction', dataset_config[\"instruction_template\"]),\n",
        "                            \"input\": row.get('th_input', ''),\n",
        "                            \"output\": row.get('th_output', ''),\n",
        "                            \"dataset_type\": dataset_name\n",
        "                        }\n",
        "\n",
        "                    # Validate sample data\n",
        "                    if (sample_data[\"instruction\"] and sample_data[\"output\"] and\n",
        "                        len(str(sample_data[\"instruction\"]).strip()) >= 5 and\n",
        "                        len(str(sample_data[\"output\"]).strip()) >= 10):\n",
        "                        samples.append(sample_data)\n",
        "\n",
        "                except Exception as e:\n",
        "                    continue\n",
        "\n",
        "            datasets[dataset_name] = samples\n",
        "            print(f\"  {dataset_name}: {len(samples)} valid samples processed\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  Error loading {dataset_name}: {str(e)}\")\n",
        "            datasets[dataset_name] = []\n",
        "\n",
        "    return datasets\n",
        "\n",
        "# FIXED: Enhanced dataset loading with proper Thai text encoding and validation\n",
        "def load_enhanced_medical_datasets():\n",
        "    \"\"\"FIX: Load medical datasets with proper Thai text handling and validation\"\"\"\n",
        "    print(\"Loading external CSV datasets...\")\n",
        "    external_datasets = load_external_csv_datasets()\n",
        "\n",
        "    # Combine all external datasets\n",
        "    all_samples = []\n",
        "    for dataset_name, samples in external_datasets.items():\n",
        "        all_samples.extend(samples)\n",
        "        print(f\"{dataset_name}: {len(samples)} samples\")\n",
        "\n",
        "    print(f\"Total external samples: {len(all_samples)}\")\n",
        "\n",
        "    # Add fallback samples if external datasets are insufficient\n",
        "    if len(all_samples) < 50:\n",
        "        print(\"Adding fallback Thai medical samples...\")\n",
        "\n",
        "        fallback_samples = [\n",
        "            {\n",
        "                \"instruction\": \"‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏Å‡∏©‡∏≤‡πÇ‡∏£‡∏Ñ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢\",\n",
        "                \"input\": \"‡πÇ‡∏£‡∏Ñ‡πÑ‡∏Ç‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏î‡∏≠‡∏≠‡∏Å‡∏°‡∏µ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£\",\n",
        "                \"output\": \"‡πÇ‡∏£‡∏Ñ‡πÑ‡∏Ç‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏î‡∏≠‡∏≠‡∏Å‡πÄ‡∏Å‡∏¥‡∏î‡∏à‡∏≤‡∏Å‡πÑ‡∏ß‡∏£‡∏±‡∏™‡πÄ‡∏î‡∏á‡∏Å‡∏µ‡πà ‡∏°‡∏µ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡πÑ‡∏Ç‡πâ‡∏™‡∏π‡∏á ‡∏õ‡∏ß‡∏î‡∏´‡∏±‡∏ß ‡∏õ‡∏ß‡∏î‡∏Å‡∏•‡πâ‡∏≤‡∏°‡πÄ‡∏ô‡∏∑‡πâ‡∏≠ ‡∏Ñ‡∏•‡∏∑‡πà‡∏ô‡πÑ‡∏™‡πâ‡∏≠‡∏≤‡πÄ‡∏à‡∏µ‡∏¢‡∏ô ‡∏ú‡∏∑‡πà‡∏ô‡πÅ‡∏î‡∏á ‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡πÄ‡∏•‡∏∑‡∏≠‡∏î‡∏≠‡∏≠‡∏Å‡∏ï‡∏≤‡∏°‡πÑ‡∏£‡∏ü‡∏±‡∏ô ‡∏´‡∏≤‡∏Å‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á‡∏≠‡∏≤‡∏à‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏Ç‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏î‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡∏ä‡πá‡∏≠‡∏Ñ ‡∏Ñ‡∏ß‡∏£‡∏î‡∏∑‡πà‡∏°‡∏ô‡πâ‡∏≥‡∏°‡∏≤‡∏Å‡πÜ ‡∏û‡∏±‡∏Å‡∏ú‡πà‡∏≠‡∏ô ‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏¢‡∏≤‡πÅ‡∏≠‡∏™‡πÑ‡∏û‡∏£‡∏±‡∏ô ‡πÅ‡∏•‡∏∞‡∏£‡∏µ‡∏ö‡∏û‡∏ö‡πÅ‡∏û‡∏ó‡∏¢‡πå‡∏ó‡∏±‡∏ô‡∏ó‡∏µ\",\n",
        "                \"dataset_type\": \"thai_medical_fallback\"\n",
        "            },\n",
        "            {\n",
        "                \"instruction\": \"‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô\",\n",
        "                \"input\": \"‡∏°‡∏µ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏õ‡∏ß‡∏î‡∏ó‡πâ‡∏≠‡∏á ‡∏ó‡πâ‡∏≠‡∏á‡πÄ‡∏™‡∏µ‡∏¢ ‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏≤ 2 ‡∏ß‡∏±‡∏ô\",\n",
        "                \"output\": \"‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏õ‡∏ß‡∏î‡∏ó‡πâ‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏ó‡πâ‡∏≠‡∏á‡πÄ‡∏™‡∏µ‡∏¢‡∏≠‡∏≤‡∏à‡πÄ‡∏Å‡∏¥‡∏î‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡πÉ‡∏ô‡∏ó‡∏≤‡∏á‡πÄ‡∏î‡∏¥‡∏ô‡∏≠‡∏≤‡∏´‡∏≤‡∏£ ‡∏Å‡∏≤‡∏£‡∏Å‡∏¥‡∏ô‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏¥‡∏© ‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ñ‡∏£‡∏µ‡∏¢‡∏î ‡∏Ñ‡∏ß‡∏£‡∏î‡∏∑‡πà‡∏°‡∏ô‡πâ‡∏≥‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏°‡∏≤‡∏Å‡πÜ ‡∏Å‡∏¥‡∏ô ORS ‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏°‡∏±‡∏ô ‡πÄ‡∏ú‡πá‡∏î ‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏≤ ‡∏Å‡∏¥‡∏ô‡∏Ç‡πâ‡∏≤‡∏ß‡∏ï‡πâ‡∏°‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏≠‡πà‡∏≠‡∏ô‡πÜ ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô 1-2 ‡∏ß‡∏±‡∏ô ‡∏´‡∏£‡∏∑‡∏≠‡∏°‡∏µ‡πÑ‡∏Ç‡πâ ‡∏Ñ‡∏ß‡∏£‡∏û‡∏ö‡πÅ‡∏û‡∏ó‡∏¢‡πå\",\n",
        "                \"dataset_type\": \"thai_medical_fallback\"\n",
        "            },\n",
        "            {\n",
        "                \"instruction\": \"‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÇ‡∏£‡∏Ñ\",\n",
        "                \"input\": \"‡∏ß‡∏¥‡∏ò‡∏µ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÇ‡∏£‡∏Ñ‡πÄ‡∏ö‡∏≤‡∏´‡∏ß‡∏≤‡∏ô\",\n",
        "                \"output\": \"‡∏Å‡∏≤‡∏£‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÇ‡∏£‡∏Ñ‡πÄ‡∏ö‡∏≤‡∏´‡∏ß‡∏≤‡∏ô: 1) ‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏õ‡∏Å‡∏ï‡∏¥ 2) ‡∏≠‡∏≠‡∏Å‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Å‡∏≤‡∏¢‡∏™‡∏°‡πà‡∏≥‡πÄ‡∏™‡∏°‡∏≠‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 30 ‡∏ô‡∏≤‡∏ó‡∏µ‡∏ï‡πà‡∏≠‡∏ß‡∏±‡∏ô 3) ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏ó‡∏≤‡∏ô‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÑ‡∏ü‡πÄ‡∏ö‡∏≠‡∏£‡πå‡∏™‡∏π‡∏á ‡∏•‡∏î‡∏ô‡πâ‡∏≥‡∏ï‡∏≤‡∏• ‡∏•‡∏î‡πÑ‡∏Ç‡∏°‡∏±‡∏ô 4) ‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏π‡∏ö‡∏ö‡∏∏‡∏´‡∏£‡∏µ‡πà‡πÅ‡∏•‡∏∞‡∏î‡∏∑‡πà‡∏°‡∏™‡∏∏‡∏£‡∏≤ 5) ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡∏õ‡∏µ 6) ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ñ‡∏£‡∏µ‡∏¢‡∏î\",\n",
        "                \"dataset_type\": \"prevention_fallback\"\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        all_samples.extend(fallback_samples)\n",
        "        print(f\"Added {len(fallback_samples)} fallback samples\")\n",
        "\n",
        "    return {\"combined_dataset\": all_samples}\n",
        "\n",
        "# Load enhanced datasets\n",
        "print(\"Loading enhanced medical datasets...\")\n",
        "medical_datasets = load_enhanced_medical_datasets()\n",
        "\n",
        "# FIX: Combine and validate all samples\n",
        "all_samples = []\n",
        "for dataset_name, samples in medical_datasets.items():\n",
        "    # Additional validation for each sample\n",
        "    valid_samples = []\n",
        "    for sample in samples:\n",
        "        if (isinstance(sample, dict) and\n",
        "            sample.get(\"instruction\") and\n",
        "            sample.get(\"output\") and\n",
        "            len(str(sample[\"instruction\"]).strip()) >= 5 and\n",
        "            len(str(sample[\"output\"]).strip()) >= 10):\n",
        "            valid_samples.append(sample)\n",
        "\n",
        "    all_samples.extend(valid_samples)\n",
        "    print(f\"{dataset_name}: {len(valid_samples)} valid samples\")\n",
        "\n",
        "print(f\"Total valid samples loaded: {len(all_samples)}\")\n",
        "\n",
        "# FIX: Enhanced train/validation/test split with proper ratios (80/10/10)\n",
        "def create_balanced_split(samples, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):\n",
        "    \"\"\"FIX: Create balanced train/validation/test split with validation\"\"\"\n",
        "    import random\n",
        "    random.seed(config[\"seed\"])\n",
        "\n",
        "    # Shuffle all samples\n",
        "    random.shuffle(samples)\n",
        "\n",
        "    total_samples = len(samples)\n",
        "    train_end = int(total_samples * train_ratio)\n",
        "    val_end = train_end + int(total_samples * val_ratio)\n",
        "\n",
        "    train_samples = samples[:train_end]\n",
        "    val_samples = samples[train_end:val_end]\n",
        "    test_samples = samples[val_end:]\n",
        "\n",
        "    return train_samples, val_samples, test_samples\n",
        "\n",
        "train_samples, val_samples, test_samples = create_balanced_split(all_samples)\n",
        "\n",
        "print(f\"Training samples: {len(train_samples)}\")\n",
        "print(f\"Validation samples: {len(val_samples)}\")\n",
        "print(f\"Test samples: {len(test_samples)}\")\n",
        "\n",
        "# Validate that we have sufficient samples\n",
        "if len(train_samples) < 10:\n",
        "    raise ValueError(\"Insufficient training samples. Need at least 10 samples.\")\n",
        "if len(val_samples) < 5:\n",
        "    raise ValueError(\"Insufficient validation samples. Need at least 5 samples.\")\n",
        "if len(test_samples) < 5:\n",
        "    raise ValueError(\"Insufficient test samples. Need at least 5 samples.\")\n",
        "\n",
        "# FIX: Enhanced tokenizer and model loading with proper encoding\n",
        "print(\"Loading OpenThaiGPT model and tokenizer with optimizations...\")\n",
        "\n",
        "# Load tokenizer with advanced settings\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    config[\"model_name\"],\n",
        "    trust_remote_code=True,\n",
        "    padding_side='right',\n",
        "    use_fast=True,\n",
        ")\n",
        "\n",
        "# FIX: Configure special tokens properly with attention mask handling\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "print(\"Loading model with advanced optimizations...\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    config[\"model_name\"],\n",
        "    quantization_config=quantization_config,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16 if config[\"bf16\"] else torch.float16,\n",
        "    use_cache=False,\n",
        ")\n",
        "\n",
        "# Enable gradient checkpointing for memory efficiency\n",
        "if config[\"use_gradient_checkpointing\"]:\n",
        "    model.gradient_checkpointing_enable()\n",
        "\n",
        "# Prepare model for k-bit training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# Apply LoRA configuration\n",
        "print(\"Applying LoRA configuration...\")\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Print trainable parameters\n",
        "def print_trainable_parameters(model):\n",
        "    \"\"\"Print the number of trainable parameters in the model.\"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(f\"Trainable params: {trainable_params:,} || All params: {all_param:,} || Trainable%: {100 * trainable_params / all_param:.2f}\")\n",
        "\n",
        "print_trainable_parameters(model)\n",
        "\n",
        "# FIX: Enhanced data preprocessing with proper Thai text handling and padding\n",
        "def format_training_sample(sample):\n",
        "    \"\"\"FIX: Format sample for training with proper Thai text encoding\"\"\"\n",
        "    instruction = str(sample.get(\"instruction\", \"\")).strip()\n",
        "    input_text = str(sample.get(\"input\", \"\")).strip()\n",
        "    output_text = str(sample.get(\"output\", \"\")).strip()\n",
        "\n",
        "    # Create Thai medical conversation format\n",
        "    if input_text:\n",
        "        prompt = f\"### ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á:\\n{instruction}\\n\\n### ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:\\n{input_text}\\n\\n### ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:\\n{output_text}\"\n",
        "    else:\n",
        "        prompt = f\"### ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á:\\n{instruction}\\n\\n### ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:\\n{output_text}\"\n",
        "\n",
        "    return prompt\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    \"\"\"FIX: Enhanced preprocessing with proper tokenization and padding\"\"\"\n",
        "    # Format all samples\n",
        "    formatted_texts = []\n",
        "    for i in range(len(examples[\"instruction\"])):\n",
        "        sample = {\n",
        "            \"instruction\": examples[\"instruction\"][i],\n",
        "            \"input\": examples.get(\"input\", [\"\"] * len(examples[\"instruction\"]))[i],\n",
        "            \"output\": examples[\"output\"][i]\n",
        "        }\n",
        "        formatted_texts.append(format_training_sample(sample))\n",
        "\n",
        "    # Tokenize with proper settings and explicit padding\n",
        "    tokenized = tokenizer(\n",
        "        formatted_texts,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=config[\"max_length\"],\n",
        "        return_tensors=None,\n",
        "        add_special_tokens=True\n",
        "    )\n",
        "\n",
        "    # Set labels for causal language modeling\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "\n",
        "    return tokenized\n",
        "\n",
        "# Convert samples to dataset format\n",
        "def samples_to_dataset_dict(samples):\n",
        "    \"\"\"Convert samples to dataset format with validation\"\"\"\n",
        "    dataset_dict = {\n",
        "        \"instruction\": [],\n",
        "        \"input\": [],\n",
        "        \"output\": [],\n",
        "        \"dataset_type\": []\n",
        "    }\n",
        "\n",
        "    for sample in samples:\n",
        "        if isinstance(sample, dict):\n",
        "            dataset_dict[\"instruction\"].append(sample.get(\"instruction\", \"\"))\n",
        "            dataset_dict[\"input\"].append(sample.get(\"input\", \"\"))\n",
        "            dataset_dict[\"output\"].append(sample.get(\"output\", \"\"))\n",
        "            dataset_dict[\"dataset_type\"].append(sample.get(\"dataset_type\", \"general\"))\n",
        "\n",
        "    return dataset_dict\n",
        "\n",
        "print(\"Creating datasets...\")\n",
        "train_dataset_dict = samples_to_dataset_dict(train_samples)\n",
        "val_dataset_dict = samples_to_dataset_dict(val_samples)\n",
        "test_dataset_dict = samples_to_dataset_dict(test_samples)\n",
        "\n",
        "# Create Dataset objects\n",
        "train_dataset = Dataset.from_dict(train_dataset_dict)\n",
        "val_dataset = Dataset.from_dict(val_dataset_dict)\n",
        "test_dataset = Dataset.from_dict(test_dataset_dict)\n",
        "\n",
        "# Apply preprocessing\n",
        "print(\"Applying preprocessing...\")\n",
        "train_dataset = train_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=train_dataset.column_names,\n",
        "    desc=\"Preprocessing training data\"\n",
        ")\n",
        "\n",
        "val_dataset = val_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=val_dataset.column_names,\n",
        "    desc=\"Preprocessing validation data\"\n",
        ")\n",
        "\n",
        "test_dataset = test_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=test_dataset.column_names,\n",
        "    desc=\"Preprocessing test data\"\n",
        ")\n",
        "\n",
        "print(f\"Training dataset size: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")\n",
        "\n",
        "# FIX: Enhanced data collator with proper padding\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,\n",
        "    pad_to_multiple_of=8,\n",
        ")\n",
        "\n",
        "# FIX: Advanced evaluation metrics\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Compute advanced metrics for medical fine-tuning evaluation\"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "\n",
        "    # Ensure predictions and labels are numpy arrays\n",
        "    if isinstance(predictions, torch.Tensor):\n",
        "        predictions = predictions.cpu().numpy()\n",
        "    if isinstance(labels, torch.Tensor):\n",
        "        labels = labels.cpu().numpy()\n",
        "\n",
        "    # Calculate perplexity\n",
        "    predictions = predictions.reshape(-1, predictions.shape[-1])\n",
        "    labels = labels.reshape(-1)\n",
        "\n",
        "    # Filter out ignored tokens (typically -100)\n",
        "    mask = labels != -100\n",
        "    if mask.sum() == 0:\n",
        "        return {\"perplexity\": float('inf'), \"eval_loss\": float('inf')}\n",
        "\n",
        "    predictions = predictions[mask]\n",
        "    labels = labels[mask]\n",
        "\n",
        "    # Calculate cross-entropy loss\n",
        "    log_probs = F.log_softmax(torch.tensor(predictions), dim=-1)\n",
        "    nll_loss = F.nll_loss(log_probs, torch.tensor(labels), reduction='mean')\n",
        "    perplexity = torch.exp(nll_loss).item()\n",
        "\n",
        "    return {\n",
        "        \"perplexity\": perplexity,\n",
        "        \"eval_loss\": nll_loss.item()\n",
        "    }\n",
        "\n",
        "# FIX: Advanced training arguments with A100 optimizations\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=config[\"output_dir\"],\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=config[\"num_epochs\"],\n",
        "    per_device_train_batch_size=config[\"batch_size\"],\n",
        "    per_device_eval_batch_size=config[\"batch_size\"],\n",
        "    gradient_accumulation_steps=config[\"gradient_accumulation_steps\"],\n",
        "    gradient_checkpointing=config[\"use_gradient_checkpointing\"],\n",
        "    learning_rate=config[\"learning_rate\"],\n",
        "    weight_decay=config[\"weight_decay\"],\n",
        "    lr_scheduler_type=config[\"lr_scheduler_type\"],\n",
        "    warmup_ratio=config[\"warmup_ratio\"],\n",
        "    logging_steps=config[\"logging_steps\"],\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=config[\"eval_steps\"],\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=config[\"save_steps\"],\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    fp16=config[\"fp16\"],\n",
        "    bf16=config[\"bf16\"],\n",
        "    dataloader_pin_memory=True,\n",
        "    dataloader_num_workers=2,\n",
        "    remove_unused_columns=False,\n",
        "    report_to=None,\n",
        "    run_name=\"enhanced_openthaigpt_medical_v4.0\",\n",
        "    seed=config[\"seed\"],\n",
        "    data_seed=config[\"seed\"],\n",
        "    group_by_length=False,\n",
        "    length_column_name=\"input_ids\",\n",
        "    ddp_find_unused_parameters=False,\n",
        "    dataloader_drop_last=False,\n",
        "    eval_accumulation_steps=1,\n",
        "    prediction_loss_only=False,\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "# FIX: Custom callback for enhanced monitoring\n",
        "class EnhancedTrainingCallback(TrainerCallback):\n",
        "    \"\"\"Enhanced callback for monitoring training progress and GPU memory\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.training_started = False\n",
        "\n",
        "    def on_train_begin(self, args, state, control, **kwargs):\n",
        "        \"\"\"Called at the beginning of training\"\"\"\n",
        "        self.training_started = True\n",
        "        print(\"üöÄ Enhanced OpenThaiGPT Medical Fine-tuning Started!\")\n",
        "        print(f\"üìä Training Configuration:\")\n",
        "        print(f\"   ‚Ä¢ Total epochs: {args.num_train_epochs}\")\n",
        "        print(f\"   ‚Ä¢ Batch size: {args.per_device_train_batch_size}\")\n",
        "        print(f\"   ‚Ä¢ Gradient accumulation: {args.gradient_accumulation_steps}\")\n",
        "        print(f\"   ‚Ä¢ Learning rate: {args.learning_rate}\")\n",
        "        print(f\"   ‚Ä¢ Effective batch size: {args.per_device_train_batch_size * args.gradient_accumulation_steps}\")\n",
        "\n",
        "        # Display GPU memory info\n",
        "        memory_info = memory_manager.get_memory_info()\n",
        "        if \"error\" not in memory_info:\n",
        "            print(f\"   ‚Ä¢ GPU Memory: {memory_info['allocated_gb']:.1f}GB allocated, {memory_info['free_gb']:.1f}GB free\")\n",
        "\n",
        "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
        "        \"\"\"Called at the beginning of each epoch\"\"\"\n",
        "        current_epoch = int(state.epoch) + 1 if state.epoch is not None else 1\n",
        "        print(f\"\\nüìà Starting Epoch {current_epoch}/{args.num_train_epochs}\")\n",
        "\n",
        "        # Clear memory at epoch start\n",
        "        memory_manager.clear_memory()\n",
        "\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        \"\"\"Called when logging\"\"\"\n",
        "        if logs and self.training_started:\n",
        "            step = logs.get('step', state.global_step)\n",
        "\n",
        "            # Training metrics\n",
        "            if 'loss' in logs:\n",
        "                print(f\"Step {step}: Loss = {logs['loss']:.4f}\")\n",
        "\n",
        "            # Evaluation metrics\n",
        "            if 'eval_loss' in logs:\n",
        "                eval_loss = logs['eval_loss']\n",
        "                perplexity = logs.get('eval_perplexity', math.exp(eval_loss))\n",
        "                print(f\"üìä Evaluation at Step {step}:\")\n",
        "                print(f\"   ‚Ä¢ Eval Loss: {eval_loss:.4f}\")\n",
        "                print(f\"   ‚Ä¢ Perplexity: {perplexity:.2f}\")\n",
        "\n",
        "                # GPU memory status\n",
        "                memory_info = memory_manager.get_memory_info()\n",
        "                if \"error\" not in memory_info:\n",
        "                    print(f\"   ‚Ä¢ GPU Memory: {memory_info['allocated_gb']:.1f}GB used\")\n",
        "\n",
        "    def on_epoch_end(self, args, state, control, **kwargs):\n",
        "        \"\"\"Called at the end of each epoch\"\"\"\n",
        "        current_epoch = int(state.epoch) if state.epoch is not None else 1\n",
        "        print(f\"‚úÖ Completed Epoch {current_epoch}\")\n",
        "\n",
        "        # Memory cleanup\n",
        "        memory_manager.clear_memory()\n",
        "\n",
        "    def on_train_end(self, args, state, control, **kwargs):\n",
        "        \"\"\"Called at the end of training\"\"\"\n",
        "        print(\"üéâ Enhanced OpenThaiGPT Medical Fine-tuning Completed!\")\n",
        "        print(f\"üìä Final Training Statistics:\")\n",
        "        print(f\"   ‚Ä¢ Total steps completed: {state.global_step}\")\n",
        "        print(f\"   ‚Ä¢ Best model saved at: {args.output_dir}\")\n",
        "\n",
        "        # Final memory cleanup\n",
        "        memory_manager.clear_memory()\n",
        "\n",
        "# FIX: Enhanced trainer with optimized settings\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[\n",
        "        EnhancedTrainingCallback(),\n",
        "        EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.01)\n",
        "    ],\n",
        ")\n",
        "\n",
        "# FIX: Pre-training system checks\n",
        "print(\"üîç Pre-training System Checks:\")\n",
        "print(f\"‚úÖ Model loaded: {model.__class__.__name__}\")\n",
        "print(f\"‚úÖ LoRA applied: {len([n for n, p in model.named_parameters() if p.requires_grad])} trainable parameters\")\n",
        "print(f\"‚úÖ Training samples: {len(train_dataset)}\")\n",
        "print(f\"‚úÖ Validation samples: {len(val_dataset)}\")\n",
        "print(f\"‚úÖ Test samples: {len(test_dataset)}\")\n",
        "\n",
        "# Display memory usage before training\n",
        "memory_info = memory_manager.get_memory_info()\n",
        "if \"error\" not in memory_info:\n",
        "    print(f\"‚úÖ GPU Memory: {memory_info['allocated_gb']:.1f}GB allocated, {memory_info['free_gb']:.1f}GB available\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è GPU not available - running on CPU\")\n",
        "\n",
        "# FIX: Enhanced training execution with error handling\n",
        "try:\n",
        "    print(\"\\nüöÄ Starting Enhanced OpenThaiGPT Medical Fine-tuning...\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Clear memory before training\n",
        "    memory_manager.clear_memory()\n",
        "\n",
        "    # Start training\n",
        "    trainer.train()\n",
        "\n",
        "    # Save the final model\n",
        "    print(\"\\nüíæ Saving final model...\")\n",
        "    final_model_path = os.path.join(config[\"output_dir\"], \"final_model\")\n",
        "    trainer.save_model(final_model_path)\n",
        "    tokenizer.save_pretrained(final_model_path)\n",
        "\n",
        "    print(f\"‚úÖ Model saved to: {final_model_path}\")\n",
        "\n",
        "    # Save training configuration\n",
        "    config_path = os.path.join(final_model_path, \"training_config.json\")\n",
        "    with open(config_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"‚úÖ Training configuration saved to: {config_path}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Training error: {str(e)}\")\n",
        "    print(\"üí° Troubleshooting suggestions:\")\n",
        "    print(\"   ‚Ä¢ Reduce batch_size in config\")\n",
        "    print(\"   ‚Ä¢ Enable gradient_checkpointing\")\n",
        "    print(\"   ‚Ä¢ Reduce max_length\")\n",
        "    print(\"   ‚Ä¢ Check GPU memory availability\")\n",
        "    raise\n",
        "\n",
        "# FIX: Post-training model testing and perplexity measurement\n",
        "def calculate_perplexity(model, tokenizer, dataset):\n",
        "    \"\"\"Calculate perplexity on test dataset\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_tokens = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(dataset)):\n",
        "            try:\n",
        "                # Get input_ids and labels from the dataset\n",
        "                input_ids = torch.tensor(dataset[i][\"input_ids\"]).unsqueeze(0).to(model.device)\n",
        "                labels = torch.tensor(dataset[i][\"labels\"]).unsqueeze(0).to(model.device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(input_ids, labels=labels)\n",
        "                loss = outputs.loss\n",
        "\n",
        "                # Count non-ignored tokens (labels != -100)\n",
        "                valid_tokens = (labels != -100).sum().item()\n",
        "\n",
        "                if valid_tokens > 0:\n",
        "                    total_loss += loss.item() * valid_tokens\n",
        "                    total_tokens += valid_tokens\n",
        "\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "    if total_tokens > 0:\n",
        "        avg_loss = total_loss / total_tokens\n",
        "        perplexity = math.exp(avg_loss)\n",
        "        return perplexity, avg_loss\n",
        "    else:\n",
        "        return float('inf'), float('inf')\n",
        "\n",
        "def test_model_generation(model, tokenizer, test_samples, num_examples=3):\n",
        "    \"\"\"Test the fine-tuned model with sample prompts\"\"\"\n",
        "    print(f\"\\nüß™ Testing Fine-tuned Model with {num_examples} examples:\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    generation_config = GenerationConfig(\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        max_new_tokens=200,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    for i in range(min(num_examples, len(test_samples))):\n",
        "        sample = test_samples[i]\n",
        "        prompt = format_training_sample(sample)\n",
        "\n",
        "        print(f\"\\nüìù Example {i+1}:\")\n",
        "        print(f\"Input: {prompt[:100]}...\")\n",
        "\n",
        "        try:\n",
        "            # Tokenize input\n",
        "            inputs = tokenizer(\n",
        "                prompt,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                max_length=config[\"max_length\"] // 2,\n",
        "                padding=True\n",
        "            ).to(model.device)\n",
        "\n",
        "            # Generate response\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(\n",
        "                    **inputs,\n",
        "                    generation_config=generation_config,\n",
        "                    use_cache=True\n",
        "                )\n",
        "\n",
        "            # Decode response\n",
        "            response = tokenizer.decode(\n",
        "                outputs[0][inputs['input_ids'].shape[1]:],\n",
        "                skip_special_tokens=True\n",
        "            )\n",
        "\n",
        "            print(f\"Generated Output: {response.strip()}\")\n",
        "            print(f\"Expected Output: {sample['output'][:100]}...\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Generation error: {str(e)}\")\n",
        "\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "# Calculate perplexity on test dataset\n",
        "print(\"\\nüìä Calculating final perplexity...\")\n",
        "test_perplexity, test_loss = calculate_perplexity(model, tokenizer, test_dataset)\n",
        "print(f\"‚úÖ Test Perplexity: {test_perplexity:.2f}\")\n",
        "print(f\"‚úÖ Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "# Test model generation\n",
        "test_model_generation(model, tokenizer, test_samples)\n",
        "\n",
        "# FIX: Final memory cleanup and summary\n",
        "memory_manager.clear_memory()\n",
        "final_memory = memory_manager.get_memory_info()\n",
        "\n",
        "print(\"\\nüéØ Training Summary:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"‚úÖ Training completed successfully!\")\n",
        "print(f\"üìä Final Test Perplexity: {test_perplexity:.2f}\")\n",
        "print(f\"üíæ Model saved to: {config['output_dir']}\")\n",
        "print(f\"üß† Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "print(f\"üìà Training samples: {len(train_dataset)}\")\n",
        "print(f\"üìä Validation samples: {len(val_dataset)}\")\n",
        "print(f\"üß™ Test samples: {len(test_dataset)}\")\n",
        "\n",
        "if \"error\" not in final_memory:\n",
        "    print(f\"üíæ GPU Memory usage: {final_memory['allocated_gb']:.1f}GB / {final_memory['total_gb']:.1f}GB\")\n",
        "\n",
        "print(\"\\nüöÄ Enhanced OpenThaiGPT Medical Fine-tuning Complete!\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "id": "hmBnqQc6oOux"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}